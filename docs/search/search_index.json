{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Omnistorage","text":"<p>Unified storage abstraction layer for Go</p> <p> </p> <p>Omnistorage provides a single interface for reading and writing to various storage backends with composable layers for compression and record framing. Inspired by rclone.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Single interface for multiple storage backends (local files, S3, cloud drives, etc.)</li> <li>Composable layers for compression (gzip, zstd) and formatting (NDJSON)</li> <li>Sync engine for file synchronization between backends (like <code>rclone sync</code>)</li> <li>Extended interface for metadata, server-side copy/move, and capability discovery</li> <li>Multi-writer for fan-out to multiple backends simultaneously</li> <li>Backend registration allowing external packages to implement backends</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>package main\n\nimport (\n    \"context\"\n    \"io\"\n    \"log\"\n\n    \"github.com/grokify/omnistorage/backend/file\"\n)\n\nfunc main() {\n    ctx := context.Background()\n\n    // Create a file backend\n    backend := file.New(file.Config{Root: \"/data\"})\n    defer backend.Close()\n\n    // Write a file\n    w, _ := backend.NewWriter(ctx, \"hello.txt\")\n    w.Write([]byte(\"Hello, World!\"))\n    w.Close()\n\n    // Read it back\n    r, _ := backend.NewReader(ctx, \"hello.txt\")\n    data, _ := io.ReadAll(r)\n    r.Close()\n\n    log.Println(string(data)) // \"Hello, World!\"\n}\n</code></pre>"},{"location":"#why-omnistorage","title":"Why Omnistorage?","text":"Challenge Omnistorage Solution Different APIs for each storage provider Single <code>Backend</code> interface Provider-specific code Backend abstraction with registration Duplicated compression/formatting logic Composable layers Hard to switch providers Configuration-driven backend selection No sync between backends rclone-inspired sync engine"},{"location":"#supported-backends","title":"Supported Backends","text":"Backend Package Status Local Filesystem <code>backend/file</code> Stable In-Memory <code>backend/memory</code> Stable S3-Compatible <code>backend/s3</code> Stable Go Channel <code>backend/channel</code> Stable Google Drive omnistorage-google Stable Google Cloud Storage Planned - Azure Blob Storage Planned -"},{"location":"#getting-started","title":"Getting Started","text":"- :material-download: **[Installation](getting-started/installation.md)**      Install omnistorage and get started in minutes  - :material-rocket-launch: **[Quick Start](getting-started/quick-start.md)**      Learn the basics with hands-on examples  - :material-book-open: **[Concepts](getting-started/concepts.md)**      Understand the architecture and design"},{"location":"#documentation-sections","title":"Documentation Sections","text":"<ul> <li>Backends - Storage backend documentation</li> <li>Sync Engine - File synchronization (rclone-like)</li> <li>Guides - How-to guides and tutorials</li> <li>Reference - API reference and interfaces</li> </ul>"},{"location":"#related-projects","title":"Related Projects","text":"<ul> <li>omnistorage-google - Google Drive and GCS backends</li> <li>rclone - Inspiration for backend coverage and sync capabilities</li> <li>go-cloud - Google's portable cloud APIs</li> <li>afero - Filesystem abstraction</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, this project adheres to Semantic Versioning, and this changelog is generated by Structured Changelog.</p>"},{"location":"changelog/#010-2026-01-08","title":"0.1.0 - 2026-01-08","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Core <code>Backend</code> interface with <code>NewWriter</code>, <code>NewReader</code>, <code>Exists</code>, <code>Delete</code>, <code>List</code>, <code>Close</code> methods</li> <li><code>RecordWriter</code> and <code>RecordReader</code> interfaces for streaming record-oriented data</li> <li><code>ExtendedBackend</code> interface with <code>Stat</code>, <code>Mkdir</code>, <code>Rmdir</code>, <code>Copy</code>, <code>Move</code>, and <code>Features</code> methods</li> <li><code>ObjectInfo</code> interface for file metadata (Size, ModTime, Hash, MimeType)</li> <li>Hash support for MD5, SHA1, SHA256, CRC32, and custom hash types</li> <li>Features struct for backend capability discovery</li> <li>Backend registry with <code>Register()</code>, <code>Open()</code>, and <code>Backends()</code> functions</li> <li>File backend (<code>backend/file</code>) for local filesystem storage</li> <li>Memory backend (<code>backend/memory</code>) for in-memory storage and testing</li> <li>S3 backend (<code>backend/s3</code>) supporting AWS S3, Cloudflare R2, MinIO, Wasabi, and other S3-compatible services</li> <li>NDJSON format (<code>format/ndjson</code>) for newline-delimited JSON record framing</li> <li>Gzip compression (<code>compress/gzip</code>) wrapper</li> <li>Zstandard compression (<code>compress/zstd</code>) wrapper</li> <li>Sync package with rclone-inspired file synchronization</li> <li><code>sync.Sync()</code> for one-way sync with optional delete</li> <li><code>sync.Copy()</code> for copying files without deleting extras</li> <li><code>sync.CopyFile()</code>, <code>sync.CopyBetweenPaths()</code>, <code>sync.CopyToPath()</code>, <code>sync.CopyFromPath()</code> convenience functions</li> <li><code>sync.TreeCopy()</code> for preserving full directory structure</li> <li><code>sync.Check()</code> for comparing files between backends</li> <li><code>sync.Verify()</code>, <code>sync.VerifyFile()</code>, <code>sync.VerifyChecksum()</code> for verification</li> <li><code>sync.VerifyWithDetails()</code> and <code>sync.VerifyAndReport()</code> for detailed verification results</li> <li><code>sync.VerifyIntegrity()</code> and <code>sync.VerifyAllIntegrity()</code> for file integrity checks</li> <li>Progress callbacks for sync operations</li> <li>Dry-run mode for sync operations</li> <li>Helper functions <code>CopyPath()</code> and <code>MovePath()</code> for cross-backend operations</li> <li><code>AsExtended()</code> helper for type-safe ExtendedBackend conversion</li> </ul>"},{"location":"roadmap/","title":"Roadmap","text":"<p>This document tracks the development roadmap for omnistorage.</p>"},{"location":"roadmap/#design-philosophy","title":"Design Philosophy","text":"<p>Omnistorage uses interface composition to support both simple and advanced use cases:</p> <ul> <li>Backend - Basic interface for read/write operations (data pipelines, simple apps)</li> <li>ExtendedBackend - Adds metadata, copy/move, directory ops (rclone-like tools)</li> </ul> <p>Applications can use just the <code>Backend</code> interface for simple operations, while advanced tools can use <code>ExtendedBackend</code> for full functionality. The basic interface remains stable and backward-compatible.</p> <pre><code>// Simple apps use Backend\nfunc SaveData(backend omnistorage.Backend, path string, data []byte) error {\n    w, _ := backend.NewWriter(ctx, path)\n    defer w.Close()\n    _, err := w.Write(data)\n    return err\n}\n\n// Advanced tools can check for extended features\nfunc CopyFile(backend omnistorage.Backend, src, dst string) error {\n    if ext, ok := backend.(omnistorage.ExtendedBackend); ok &amp;&amp; ext.Features().Copy {\n        return ext.Copy(ctx, src, dst) // Server-side copy\n    }\n    return omnistorage.CopyPath(ctx, backend, src, backend, dst) // Fallback\n}\n</code></pre>"},{"location":"roadmap/#phase-1-foundation-mvp","title":"Phase 1: Foundation (MVP) \u2705","text":"<p>Core interfaces and essential implementations.</p>"},{"location":"roadmap/#core-package","title":"Core Package","text":"<ul> <li>[x] <code>interfaces.go</code> - Backend, RecordWriter, RecordReader interfaces</li> <li>[x] <code>options.go</code> - WriterOption, ReaderOption, configs</li> <li>[x] <code>registry.go</code> - Register(), Open(), Backends()</li> <li>[x] <code>errors.go</code> - Common error types (ErrNotFound, etc.)</li> </ul>"},{"location":"roadmap/#format-layer","title":"Format Layer","text":"<ul> <li>[x] <code>format/ndjson/writer.go</code> - NDJSON RecordWriter</li> <li>[x] <code>format/ndjson/reader.go</code> - NDJSON RecordReader</li> <li>[x] <code>format/ndjson/ndjson_test.go</code> - Tests</li> </ul>"},{"location":"roadmap/#compression-layer","title":"Compression Layer","text":"<ul> <li>[x] <code>compress/gzip/writer.go</code> - Gzip io.WriteCloser wrapper</li> <li>[x] <code>compress/gzip/reader.go</code> - Gzip io.ReadCloser wrapper</li> <li>[x] <code>compress/gzip/gzip_test.go</code> - Tests</li> </ul>"},{"location":"roadmap/#backend-layer","title":"Backend Layer","text":"<ul> <li>[x] <code>backend/file/backend.go</code> - Local filesystem backend</li> <li>[x] <code>backend/file/file_test.go</code> - Tests</li> <li>[x] <code>backend/memory/backend.go</code> - In-memory backend</li> <li>[x] <code>backend/memory/memory_test.go</code> - Tests</li> </ul>"},{"location":"roadmap/#testing","title":"Testing","text":"<ul> <li>[x] Conformance test suite for backends</li> <li>[x] Integration tests</li> </ul>"},{"location":"roadmap/#documentation","title":"Documentation","text":"<ul> <li>[x] README.md with usage examples</li> <li>[x] GoDoc comments on all public APIs</li> </ul>"},{"location":"roadmap/#phase-2-extended-interfaces-rclone-like","title":"Phase 2: Extended Interfaces (rclone-like) \u2705","text":"<p>Extended backend interface for metadata, directory operations, and server-side operations.</p>"},{"location":"roadmap/#core-interfaces","title":"Core Interfaces","text":"<ul> <li>[x] <code>object_info.go</code> - ObjectInfo interface (Size, ModTime, Hash, MimeType)</li> <li>[x] <code>hash.go</code> - HashType enum and hash utilities</li> <li>[x] <code>features.go</code> - Features struct for capability discovery</li> <li>[x] <code>extended.go</code> - ExtendedBackend interface embedding Backend</li> </ul>"},{"location":"roadmap/#extended-backend-interface","title":"Extended Backend Interface","text":"<pre><code>type ExtendedBackend interface {\n    Backend\n    Stat(ctx context.Context, path string) (ObjectInfo, error)\n    Mkdir(ctx context.Context, path string) error\n    Rmdir(ctx context.Context, path string) error\n    Copy(ctx context.Context, src, dst string) error\n    Move(ctx context.Context, src, dst string) error\n    Features() Features\n}\n</code></pre>"},{"location":"roadmap/#update-file-backend","title":"Update File Backend","text":"<ul> <li>[x] <code>backend/file/extended.go</code> - ExtendedBackend implementation for file</li> <li>[x] <code>backend/file/extended_test.go</code> - Tests</li> </ul>"},{"location":"roadmap/#utilities","title":"Utilities","text":"<ul> <li>[x] <code>copy.go</code> - CopyPath helper (fallback for backends without Copy)</li> <li>[x] <code>move.go</code> - MovePath helper (fallback for backends without Move)</li> </ul>"},{"location":"roadmap/#phase-3-cloud-storage","title":"Phase 3: Cloud Storage \u2705","text":"<p>S3-compatible and channel backends with extended interface support.</p>"},{"location":"roadmap/#backends","title":"Backends","text":"<ul> <li>[x] <code>backend/s3/backend.go</code> - S3-compatible backend (AWS, R2, MinIO, Wasabi, etc.)</li> <li>[x] <code>backend/s3/extended.go</code> - ExtendedBackend implementation (integrated in backend.go)</li> <li>[x] <code>backend/s3/options.go</code> - S3-specific configuration</li> <li>[x] <code>backend/s3/multipart.go</code> - Multipart upload support (via AWS SDK manager)</li> <li>[x] <code>backend/s3/s3_test.go</code> - Tests (with LocalStack/MinIO)</li> <li>[x] <code>backend/channel/backend.go</code> - Go channel backend (inter-goroutine communication)</li> <li>[x] <code>backend/channel/backend_test.go</code> - Tests (15 tests)</li> </ul>"},{"location":"roadmap/#utilities_1","title":"Utilities","text":"<ul> <li>[x] <code>multi/writer.go</code> - Fan-out to multiple backends (WriteAll, WriteBestEffort, WriteQuorum modes)</li> <li>[x] <code>multi/writer_test.go</code> - Tests (14 tests)</li> </ul>"},{"location":"roadmap/#compression","title":"Compression","text":"<ul> <li>[x] <code>compress/zstd/writer.go</code> - Zstandard compression</li> <li>[x] <code>compress/zstd/reader.go</code></li> <li>[x] <code>compress/zstd/zstd_test.go</code></li> </ul>"},{"location":"roadmap/#format","title":"Format","text":"<ul> <li>[ ] <code>format/lengthprefix/writer.go</code> - Length-prefixed binary framing</li> <li>[ ] <code>format/lengthprefix/reader.go</code></li> <li>[ ] <code>format/lengthprefix/lengthprefix_test.go</code></li> </ul>"},{"location":"roadmap/#phase-4-cloud-providers-partial","title":"Phase 4: Cloud Providers (Partial) \u2705","text":"<p>Major cloud provider backends.</p>"},{"location":"roadmap/#backends_1","title":"Backends","text":"<ul> <li>[ ] <code>backend/gcs/backend.go</code> - Google Cloud Storage</li> <li>[ ] <code>backend/gcs/extended.go</code> - ExtendedBackend implementation</li> <li>[ ] <code>backend/azure/backend.go</code> - Azure Blob Storage</li> <li>[ ] <code>backend/azure/extended.go</code> - ExtendedBackend implementation</li> <li>[x] <code>backend/sftp/backend.go</code> - SFTP with ExtendedBackend support</li> <li>[x] <code>backend/sftp/options.go</code> - SFTP configuration (password, key file, known_hosts)</li> <li>[ ] <code>backend/ftp/backend.go</code> - FTP</li> <li>[ ] <code>backend/webdav/backend.go</code> - WebDAV</li> </ul>"},{"location":"roadmap/#testing_1","title":"Testing","text":"<ul> <li>[ ] Integration tests with real cloud services (CI secrets)</li> <li>[ ] Performance benchmarks</li> </ul>"},{"location":"roadmap/#phase-5-sync-engine-rclone-inspired","title":"Phase 5: Sync Engine (rclone-inspired) \u2705","text":"<p>File synchronization and comparison tools. Inspired by rclone's most-used features.</p>"},{"location":"roadmap/#design-philosophy_1","title":"Design Philosophy","text":"<p>The sync engine provides rclone-like functionality as a Go library:</p> <ul> <li>Sync - Make destination match source (like <code>rclone sync</code>)</li> <li>Copy - Copy files without deleting extras (like <code>rclone copy</code>)</li> <li>Move - Move files from source to destination (like <code>rclone move</code>)</li> <li>Check - Verify files match between backends (like <code>rclone check</code>)</li> <li>Verify - Verify file integrity (like <code>rclone cryptcheck</code>)</li> </ul>"},{"location":"roadmap/#core-sync-package","title":"Core Sync Package \u2705","text":"<ul> <li>[x] <code>sync/sync.go</code> - Sync, CopyDir, Move, MoveFile implementations</li> <li>[x] <code>sync/copy.go</code> - Copy function for single files/directories</li> <li>[x] <code>sync/check.go</code> - Check and Diff functions</li> <li>[x] <code>sync/verify.go</code> - Verify and VerifyFile functions</li> <li>[x] <code>sync/options.go</code> - Sync options, results, and types</li> <li>[x] <code>sync/ratelimit.go</code> - Token bucket rate limiting</li> <li>[x] <code>sync/retry.go</code> - Retry with exponential backoff</li> <li>[x] <code>sync/sync_test.go</code> - Tests (80+ tests)</li> </ul> <pre><code>// Core sync API\ntype Options struct {\n    DeleteExtra      bool              // Delete files in dst not in src\n    DryRun           bool              // Report changes without making them\n    Checksum         bool              // Compare by checksum vs modtime/size\n    Progress         func(Progress)    // Progress callback\n    Concurrency      int               // Parallel transfers (default: 4)\n    BandwidthLimit   int64             // Rate limit in bytes/second\n    Retry            *RetryConfig      // Retry configuration\n    Filter           *filter.Filter    // Include/exclude filters\n    PreserveMetadata *MetadataOptions  // Metadata preservation options\n}\n\nfunc Sync(ctx, src, dst, srcPath, dstPath, opts) (*Result, error)\nfunc Copy(ctx, src, dst, srcPath, dstPath, opts) (*Result, error)\nfunc Move(ctx, src, dst, srcPath, dstPath, opts) (*Result, error)\nfunc Check(ctx, src, dst, srcPath, dstPath, opts) (*CheckResult, error)\nfunc Verify(ctx, src, dst, srcPath, dstPath, opts) (*VerifyResult, error)\n</code></pre>"},{"location":"roadmap/#comparison-tools","title":"Comparison Tools \u2705","text":"<ul> <li>[x] <code>sync/check.go</code> - Check function (compare src/dst)</li> <li>[x] <code>sync/check.go</code> - Diff function (show differences)</li> <li>[ ] <code>sync/dedupe.go</code> - Find duplicate files (future)</li> </ul>"},{"location":"roadmap/#transfer-controls","title":"Transfer Controls \u2705","text":"<ul> <li>[x] Parallel transfers - <code>Options{Concurrency: N}</code></li> <li>[x] Bandwidth limiting - <code>Options{BandwidthLimit: N}</code> (token bucket)</li> <li>[x] Retry with backoff - <code>Options{Retry: &amp;RetryConfig{}}</code></li> <li>[x] Progress callbacks - <code>Options{Progress: func(Progress)}</code></li> </ul>"},{"location":"roadmap/#filtering","title":"Filtering \u2705","text":"<ul> <li>[x] <code>sync/filter/filter.go</code> - Filter implementation with all features</li> <li>[x] Include/Exclude patterns - <code>filter.Include(\"*.json\")</code>, <code>filter.Exclude(\"*.tmp\")</code></li> <li>[x] Size filters - <code>filter.MinSize(100)</code>, <code>filter.MaxSize(1*MB)</code></li> <li>[x] Age filters - <code>filter.MinAge(24*time.Hour)</code>, <code>filter.MaxAge(7*24*time.Hour)</code></li> <li>[x] Filter from file - <code>filter.FromFile(\"filters.txt\")</code></li> <li>[x] <code>sync/filter/filter_test.go</code> - Tests (14 tests)</li> </ul>"},{"location":"roadmap/#phase-6-consumer-cloud-protocols","title":"Phase 6: Consumer Cloud &amp; Protocols","text":"<p>Consumer cloud storage and messaging.</p> <p>Note: Cloud provider backends with large SDK dependencies are in separate repos to keep the core omnistorage package lightweight:</p> <ul> <li>Google backends \u2192 github.com/grokify/omnistorage-google</li> <li>Google Drive \u2705</li> <li>Google Cloud Storage (planned)</li> </ul>"},{"location":"roadmap/#cloud-drive-backends","title":"Cloud Drive Backends","text":"<ul> <li>[ ] <code>backend/dropbox/backend.go</code> - Dropbox</li> <li>[ ] <code>backend/dropbox/extended.go</code> - ExtendedBackend implementation</li> <li>[x] Google Drive - See <code>github.com/grokify/omnistorage-google/backend/drive</code></li> <li>[ ] Google Cloud Storage - See <code>github.com/grokify/omnistorage-google/backend/gcs</code></li> <li>[ ] <code>backend/onedrive/backend.go</code> - Microsoft OneDrive</li> <li>[ ] <code>backend/onedrive/extended.go</code> - ExtendedBackend implementation</li> <li>[ ] <code>backend/box/backend.go</code> - Box</li> <li>[ ] <code>backend/icloud/backend.go</code> - iCloud Drive</li> </ul>"},{"location":"roadmap/#protocol-backends","title":"Protocol Backends","text":"<ul> <li>[ ] <code>backend/http/backend.go</code> - HTTP POST/PUT (webhooks)</li> <li>[ ] <code>backend/smb/backend.go</code> - SMB/CIFS</li> </ul>"},{"location":"roadmap/#messaging-backends","title":"Messaging Backends","text":"<ul> <li>[ ] <code>backend/kafka/backend.go</code> - Apache Kafka</li> <li>[ ] <code>backend/nats/backend.go</code> - NATS</li> <li>[ ] <code>backend/sqs/backend.go</code> - AWS SQS</li> <li>[ ] <code>backend/pubsub/backend.go</code> - Google Pub/Sub</li> </ul>"},{"location":"roadmap/#phase-7-security-authentication","title":"Phase 7: Security &amp; Authentication","text":"<p>Security, credential management, and OAuth support.</p>"},{"location":"roadmap/#credential-management","title":"Credential Management","text":"<ul> <li>[ ] <code>auth/credentials.go</code> - Credential interface</li> <li>[ ] <code>auth/omnivault/provider.go</code> - Integration with github.com/agentplexus/omnivault</li> <li>[ ] <code>auth/env/provider.go</code> - Environment variable credentials</li> <li>[ ] <code>auth/file/provider.go</code> - File-based credentials (encrypted)</li> <li>[ ] <code>auth/iam/provider.go</code> - AWS/GCP IAM role support</li> </ul>"},{"location":"roadmap/#oauth-support","title":"OAuth Support","text":"<ul> <li>[ ] <code>auth/oauth/provider.go</code> - OAuth credential provider</li> <li>[ ] <code>auth/oauth/goauth.go</code> - Integration with github.com/grokify/goauth</li> <li>[ ] <code>auth/oauth/token.go</code> - Token refresh handling</li> <li>[ ] <code>auth/oauth/oauth_test.go</code> - Tests</li> </ul>"},{"location":"roadmap/#signed-urls","title":"Signed URLs","text":"<ul> <li>[ ] <code>signedurl/signedurl.go</code> - Signed URL generation interface</li> <li>[ ] <code>signedurl/s3.go</code> - S3 presigned URLs</li> <li>[ ] <code>signedurl/gcs.go</code> - GCS signed URLs</li> </ul>"},{"location":"roadmap/#phase-8-observability","title":"Phase 8: Observability","text":"<p>Logging, metrics, and tracing support.</p>"},{"location":"roadmap/#logging","title":"Logging","text":"<ul> <li>[ ] <code>observe/logging/logger.go</code> - slog integration</li> <li>[ ] <code>observe/logging/context.go</code> - Context-aware logging</li> <li>[ ] <code>observe/logging/middleware.go</code> - Logging middleware for backends</li> </ul>"},{"location":"roadmap/#metrics","title":"Metrics","text":"<ul> <li>[ ] <code>observe/metrics/metrics.go</code> - Metrics interface</li> <li>[ ] <code>observe/metrics/prometheus.go</code> - Prometheus exporter</li> <li>[ ] <code>observe/metrics/otel.go</code> - OpenTelemetry metrics</li> </ul>"},{"location":"roadmap/#tracing","title":"Tracing","text":"<ul> <li>[ ] <code>observe/tracing/tracing.go</code> - Tracing interface</li> <li>[ ] <code>observe/tracing/otel.go</code> - OpenTelemetry tracing</li> <li>[ ] <code>observe/tracing/middleware.go</code> - Tracing middleware</li> </ul>"},{"location":"roadmap/#audit","title":"Audit","text":"<ul> <li>[ ] <code>observe/audit/audit.go</code> - Audit logging interface</li> <li>[ ] <code>observe/audit/file.go</code> - File-based audit log</li> <li>[ ] <code>observe/audit/structured.go</code> - Structured audit events</li> </ul>"},{"location":"roadmap/#phase-9-middleware-utilities","title":"Phase 9: Middleware &amp; Utilities","text":"<p>Advanced features and middleware.</p>"},{"location":"roadmap/#middleware","title":"Middleware","text":"<ul> <li>[ ] <code>middleware/buffer/writer.go</code> - Configurable buffering</li> <li>[ ] <code>middleware/retry/writer.go</code> - Retry with exponential backoff</li> <li>[ ] <code>middleware/metrics/writer.go</code> - Observability (bytes, latency, errors)</li> <li>[ ] <code>middleware/ratelimit/writer.go</code> - Rate limiting</li> <li>[ ] <code>middleware/encrypt/writer.go</code> - Client-side encryption</li> <li>[ ] <code>middleware/cache/cache.go</code> - Local cache for remote backends</li> </ul>"},{"location":"roadmap/#utilities_2","title":"Utilities","text":"<ul> <li>[ ] <code>multi/reader.go</code> - Read from multiple sources (merge/concat)</li> <li>[ ] <code>tee/writer.go</code> - Write to backend and return copy</li> </ul>"},{"location":"roadmap/#format_1","title":"Format","text":"<ul> <li>[ ] <code>format/csv/writer.go</code> - CSV format</li> <li>[ ] <code>format/csv/reader.go</code></li> <li>[ ] <code>format/parquet/writer.go</code> - Parquet format (future)</li> </ul>"},{"location":"roadmap/#phase-10-testing-infrastructure","title":"Phase 10: Testing Infrastructure","text":"<p>Testing tools and frameworks for omnistorage.</p>"},{"location":"roadmap/#mock-backend","title":"Mock Backend","text":"<ul> <li>[ ] <code>backend/mock/backend.go</code> - Configurable mock backend</li> <li>[ ] <code>backend/mock/expectations.go</code> - Set expectations for testing</li> <li>[ ] <code>backend/mock/assertions.go</code> - Assert operations occurred</li> </ul>"},{"location":"roadmap/#conformance-suite","title":"Conformance Suite","text":"<ul> <li>[ ] <code>testing/conformance/suite.go</code> - Backend conformance test suite</li> <li>[ ] <code>testing/conformance/basic.go</code> - Basic operations tests</li> <li>[ ] <code>testing/conformance/extended.go</code> - ExtendedBackend tests</li> <li>[ ] <code>testing/conformance/concurrent.go</code> - Concurrency tests</li> </ul>"},{"location":"roadmap/#benchmarks","title":"Benchmarks","text":"<ul> <li>[ ] <code>testing/benchmark/benchmark.go</code> - Benchmark framework</li> <li>[ ] <code>testing/benchmark/throughput.go</code> - Throughput benchmarks</li> <li>[ ] <code>testing/benchmark/latency.go</code> - Latency benchmarks</li> </ul>"},{"location":"roadmap/#phase-11-configuration-events","title":"Phase 11: Configuration &amp; Events","text":"<p>Configuration management and event notifications.</p>"},{"location":"roadmap/#configuration","title":"Configuration","text":"<ul> <li>[ ] <code>config/config.go</code> - Configuration interface</li> <li>[ ] <code>config/yaml.go</code> - YAML configuration loader</li> <li>[ ] <code>config/toml.go</code> - TOML configuration loader</li> <li>[ ] <code>config/env.go</code> - Environment variable overrides</li> <li>[ ] <code>config/profile.go</code> - Profile/remote management</li> </ul>"},{"location":"roadmap/#events","title":"Events","text":"<ul> <li>[ ] <code>events/events.go</code> - Event interface</li> <li>[ ] <code>events/watcher.go</code> - Watch for file changes</li> <li>[ ] <code>events/webhook.go</code> - Webhook notifications</li> <li>[ ] <code>events/channel.go</code> - Go channel events</li> </ul>"},{"location":"roadmap/#versioning","title":"Versioning","text":"<ul> <li>[ ] <code>versioning/versioning.go</code> - Object versioning interface</li> <li>[ ] <code>versioning/lifecycle.go</code> - Lifecycle policies</li> <li>[ ] <code>versioning/softdelete.go</code> - Soft delete support</li> </ul>"},{"location":"roadmap/#phase-12-extended-backend-coverage","title":"Phase 12: Extended Backend Coverage","text":"<p>Additional backends inspired by rclone.</p>"},{"location":"roadmap/#object-storage","title":"Object Storage","text":"<ul> <li>[ ] Backblaze B2</li> <li>[ ] DigitalOcean Spaces</li> <li>[ ] Linode Object Storage</li> <li>[ ] Oracle Cloud Storage</li> <li>[ ] Alibaba Cloud OSS</li> <li>[ ] Tencent Cloud COS</li> <li>[ ] Huawei Cloud OBS</li> </ul>"},{"location":"roadmap/#cloud-drives","title":"Cloud Drives","text":"<ul> <li>[ ] pCloud</li> <li>[ ] MEGA</li> <li>[ ] Yandex Disk</li> <li>[ ] Mail.ru Cloud</li> <li>[ ] Jottacloud</li> <li>[ ] Koofr</li> <li>[ ] HiDrive</li> </ul>"},{"location":"roadmap/#enterprise","title":"Enterprise","text":"<ul> <li>[ ] Citrix ShareFile</li> <li>[ ] Enterprise File Fabric</li> <li>[ ] Nextcloud/ownCloud</li> </ul>"},{"location":"roadmap/#specialized","title":"Specialized","text":"<ul> <li>[ ] Internet Archive</li> <li>[ ] HDFS (Hadoop)</li> <li>[ ] Storj (decentralized)</li> </ul>"},{"location":"roadmap/#phase-13-cli-tool","title":"Phase 13: CLI Tool","text":"<p>Command-line interface similar to rclone for operations and testing.</p>"},{"location":"roadmap/#core-cli","title":"Core CLI","text":"<ul> <li>[ ] <code>cmd/omnistorage/main.go</code> - CLI entry point</li> <li>[ ] <code>cmd/omnistorage/ls.go</code> - List files</li> <li>[ ] <code>cmd/omnistorage/cp.go</code> - Copy files</li> <li>[ ] <code>cmd/omnistorage/mv.go</code> - Move files</li> <li>[ ] <code>cmd/omnistorage/rm.go</code> - Remove files</li> <li>[ ] <code>cmd/omnistorage/sync.go</code> - Sync directories</li> <li>[ ] <code>cmd/omnistorage/check.go</code> - Verify files match</li> <li>[ ] <code>cmd/omnistorage/config.go</code> - Manage backend configurations</li> </ul>"},{"location":"roadmap/#cli-features","title":"CLI Features","text":"<ul> <li>[ ] Progress bars for transfers</li> <li>[ ] Verbose/quiet output modes</li> <li>[ ] JSON output for scripting</li> <li>[ ] Interactive configuration wizard</li> </ul>"},{"location":"roadmap/#phase-14-advanced-sync-features-partial","title":"Phase 14: Advanced Sync Features (Partial) \u2705","text":"<p>Advanced synchronization capabilities.</p>"},{"location":"roadmap/#bidirectional-sync","title":"Bidirectional Sync \u2705","text":"<ul> <li>[x] <code>sync/bisync.go</code> - Two-way synchronization with conflict resolution</li> <li>[x] <code>sync/bisync.go</code> - ConflictStrategy: NewerWins, LargerWins, SourceWins, DestWins, KeepBoth, Skip, Error</li> <li>[x] <code>sync/bisync_test.go</code> - Tests</li> </ul>"},{"location":"roadmap/#incrementalsnapshot","title":"Incremental/Snapshot","text":"<ul> <li>[ ] <code>sync/snapshot.go</code> - Point-in-time snapshots</li> <li>[ ] <code>sync/incremental.go</code> - Incremental backup support</li> </ul>"},{"location":"roadmap/#future-considerations","title":"Future Considerations","text":"<p>Items for future evaluation.</p>"},{"location":"roadmap/#features","title":"Features","text":"<ul> <li>[ ] Async/batch write support at interface level</li> <li>[ ] Streaming uploads with progress callbacks</li> <li>[ ] Connection pooling configuration</li> <li>[ ] FUSE mount support (separate package)</li> <li>[ ] WebDAV/HTTP server for backends</li> </ul>"},{"location":"roadmap/#tooling","title":"Tooling","text":"<ul> <li>[ ] Backend health check utility</li> <li>[ ] Migration tool between backends</li> <li>[ ] Performance profiler</li> </ul>"},{"location":"roadmap/#integration","title":"Integration","text":"<ul> <li>[ ] Example integrations</li> <li>[ ] OpenTelemetry tracing</li> <li>[ ] Prometheus metrics exporter</li> </ul>"},{"location":"roadmap/#version-history","title":"Version History","text":"Version Date Milestone v0.1.0 2026-01-10 Initial release: Phases 1-5 complete + SFTP + Bisync v0.2.0 TBD Phase 6 partial (Consumer cloud backends) v0.3.0 TBD Phase 7 (Security &amp; authentication) v0.4.0 TBD Phase 8 (Observability) v1.0.0 TBD Stable API, production ready v1.1.0 TBD CLI tool (Phase 13)"},{"location":"roadmap/#v010-includes","title":"v0.1.0 Includes","text":"<ul> <li>Core: Backend, ExtendedBackend, RecordWriter/Reader interfaces</li> <li>Backends: File, Memory, S3, SFTP, Channel</li> <li>Compression: Gzip, Zstd</li> <li>Format: NDJSON</li> <li>Sync: Sync, Copy, Move, Check, Verify, Bisync with filtering and rate limiting</li> <li>Multi-writer: Fan-out to multiple backends</li> </ul>"},{"location":"roadmap/#contributing","title":"Contributing","text":"<p>We welcome contributions! Priority areas:</p> <ol> <li>New backends - Follow <code>backend/file</code> as a template</li> <li>Tests - Especially integration tests with real services</li> <li>Documentation - Examples, guides, GoDoc</li> <li>Bug fixes - Issues labeled <code>good first issue</code></li> </ol> <p>See CONTRIBUTING.md for guidelines.</p>"},{"location":"backends/","title":"Backends Overview","text":"<p>Omnistorage supports multiple storage backends through a unified interface. Each backend implements the core <code>Backend</code> interface, and many also implement <code>ExtendedBackend</code> for additional capabilities.</p>"},{"location":"backends/#available-backends","title":"Available Backends","text":"Backend Package Extended Description File <code>backend/file</code> Yes Local filesystem Memory <code>backend/memory</code> Yes In-memory storage S3 <code>backend/s3</code> Yes S3-compatible storage SFTP <code>backend/sftp</code> Yes SSH file transfer Channel <code>backend/channel</code> No Go channel for streaming"},{"location":"backends/#external-backends","title":"External Backends","text":"<p>Some backends are in separate repositories to minimize dependencies:</p> Backend Repository Description Google Drive omnistorage-google Google Drive API Google Cloud Storage omnistorage-google GCS (planned)"},{"location":"backends/#backend-capabilities","title":"Backend Capabilities","text":"<p>Each backend has different capabilities:</p> Feature File Memory S3 SFTP Channel Read/Write Yes Yes Yes Yes Yes Stat Yes Yes Yes Yes No Copy Yes Yes Yes Yes No Move Yes Yes Yes Yes No Mkdir Yes Yes Yes Yes No Range Read Yes No Yes Yes No Streaming Yes Yes Yes Yes Yes"},{"location":"backends/#using-the-registry","title":"Using the Registry","text":"<p>Backends register themselves automatically when imported:</p> <pre><code>import (\n    \"github.com/grokify/omnistorage\"\n\n    // Side-effect imports register backends\n    _ \"github.com/grokify/omnistorage/backend/file\"\n    _ \"github.com/grokify/omnistorage/backend/s3\"\n)\n\n// Open by name\nbackend, err := omnistorage.Open(\"file\", map[string]string{\n    \"root\": \"/data\",\n})\n</code></pre>"},{"location":"backends/#configuration-driven-selection","title":"Configuration-Driven Selection","text":"<p>Select backends at runtime from configuration:</p> <pre><code>backendType := os.Getenv(\"STORAGE_BACKEND\")\nconfig := map[string]string{\n    \"root\":     os.Getenv(\"STORAGE_ROOT\"),\n    \"bucket\":   os.Getenv(\"STORAGE_BUCKET\"),\n    \"region\":   os.Getenv(\"STORAGE_REGION\"),\n    \"endpoint\": os.Getenv(\"STORAGE_ENDPOINT\"),\n}\n\nbackend, err := omnistorage.Open(backendType, config)\n</code></pre>"},{"location":"backends/#implementing-a-custom-backend","title":"Implementing a Custom Backend","text":"<p>See Custom Backend Guide for how to implement your own backend.</p>"},{"location":"backends/channel/","title":"Channel Backend","text":"<p>The channel backend uses Go channels for inter-goroutine communication, useful for pipeline processing and streaming data between goroutines.</p>"},{"location":"backends/channel/#installation","title":"Installation","text":"<pre><code>import \"github.com/grokify/omnistorage/backend/channel\"\n</code></pre>"},{"location":"backends/channel/#usage","title":"Usage","text":""},{"location":"backends/channel/#basic-usage","title":"Basic Usage","text":"<pre><code>backend := channel.New()\n\n// Producer goroutine\ngo func() {\n    w, _ := backend.NewWriter(ctx, \"events\")\n    w.Write([]byte(\"event1\"))\n    w.Write([]byte(\"event2\"))\n    w.Close() // Signals end of stream\n}()\n\n// Consumer goroutine\nr, _ := backend.NewReader(ctx, \"events\")\nfor {\n    buf := make([]byte, 1024)\n    n, err := r.Read(buf)\n    if err == io.EOF {\n        break\n    }\n    process(buf[:n])\n}\nr.Close()\n</code></pre>"},{"location":"backends/channel/#using-the-registry","title":"Using the Registry","text":"<pre><code>import (\n    \"github.com/grokify/omnistorage\"\n    _ \"github.com/grokify/omnistorage/backend/channel\"\n)\n\nbackend, _ := omnistorage.Open(\"channel\", map[string]string{\n    \"buffer_size\": \"100\",\n    \"persistent\":  \"true\",\n})\n</code></pre>"},{"location":"backends/channel/#configuration","title":"Configuration","text":""},{"location":"backends/channel/#options","title":"Options","text":"<pre><code>// Custom buffer size\nbackend := channel.New(channel.WithBufferSize(50))\n\n// Persistent mode - buffers data for late readers\nbackend := channel.New(channel.WithPersistence(true))\n\n// Combined\nbackend := channel.New(\n    channel.WithBufferSize(100),\n    channel.WithPersistence(true),\n)\n</code></pre>"},{"location":"backends/channel/#registry-config","title":"Registry Config","text":"Key Description Default <code>buffer_size</code> Channel buffer size 100 <code>persistent</code> Buffer data for late readers false"},{"location":"backends/channel/#features","title":"Features","text":"Feature Supported Notes Read/Write Yes Via Go channels Stat No Not applicable Copy/Move No Not applicable Broadcast Yes Send to multiple channels"},{"location":"backends/channel/#use-cases","title":"Use Cases","text":""},{"location":"backends/channel/#pipeline-processing","title":"Pipeline Processing","text":"<pre><code>backend := channel.New()\n\n// Stage 1: Read from source\ngo func() {\n    w, _ := backend.NewWriter(ctx, \"stage1\")\n    for _, item := range sourceData {\n        w.Write(processStage1(item))\n    }\n    w.Close()\n}()\n\n// Stage 2: Transform\ngo func() {\n    r, _ := backend.NewReader(ctx, \"stage1\")\n    w, _ := backend.NewWriter(ctx, \"stage2\")\n\n    for {\n        buf := make([]byte, 4096)\n        n, err := r.Read(buf)\n        if err == io.EOF {\n            break\n        }\n        w.Write(processStage2(buf[:n]))\n    }\n    r.Close()\n    w.Close()\n}()\n\n// Stage 3: Consume\nr, _ := backend.NewReader(ctx, \"stage2\")\n// ...\n</code></pre>"},{"location":"backends/channel/#test-fixtures","title":"Test Fixtures","text":"<pre><code>func TestProcessor(t *testing.T) {\n    backend := channel.New(channel.WithPersistence(true))\n\n    // Setup test data\n    w, _ := backend.NewWriter(ctx, \"input\")\n    w.Write([]byte(\"test data\"))\n    w.Close()\n\n    // Readers can now get the data even after writer closed\n    r, _ := backend.NewReader(ctx, \"input\")\n    // ...\n}\n</code></pre>"},{"location":"backends/channel/#event-fan-out","title":"Event Fan-Out","text":"<pre><code>backend := channel.New()\n\n// Create subscriber channels\nfor i := 0; i &lt; 3; i++ {\n    path := fmt.Sprintf(\"subscriber/%d\", i)\n    go func(p string) {\n        r, _ := backend.NewReader(ctx, p)\n        defer r.Close()\n        // Handle events...\n    }(path)\n}\n\n// Broadcast events to all subscribers\nbackend.Broadcast(ctx, \"subscriber/\", []byte(\"event data\"))\n</code></pre>"},{"location":"backends/channel/#broadcast","title":"Broadcast","text":"<p>Send data to all channels matching a prefix:</p> <pre><code>// Send to all channels starting with \"events/\"\nerr := backend.Broadcast(ctx, \"events/\", []byte(\"broadcast message\"))\n</code></pre>"},{"location":"backends/channel/#persistent-mode","title":"Persistent Mode","text":"<p>When persistence is enabled, data written to a channel is buffered and replayed to new readers:</p> <pre><code>backend := channel.New(channel.WithPersistence(true))\n\n// Write first\nw, _ := backend.NewWriter(ctx, \"data\")\nw.Write([]byte(\"message 1\"))\nw.Write([]byte(\"message 2\"))\n// Don't close yet\n\n// Reader connects and gets buffered data\nr, _ := backend.NewReader(ctx, \"data\")\n// Reads \"message 1\", \"message 2\"\n</code></pre> <p>Memory Usage</p> <p>Persistent mode stores all data in memory. Use with caution for high-volume data.</p>"},{"location":"backends/channel/#channel-count","title":"Channel Count","text":"<p>Check the number of active channels:</p> <pre><code>count := backend.ChannelCount()\nfmt.Printf(\"Active channels: %d\\n\", count)\n</code></pre>"},{"location":"backends/channel/#best-practices","title":"Best Practices","text":"<ol> <li>Close writers - Signals EOF to readers</li> <li>Use buffered channels - Default buffer of 100 prevents blocking</li> <li>Use persistent mode for testing - Allows readers to connect after writes</li> <li>Use Broadcast for fan-out - More efficient than multiple writes</li> </ol>"},{"location":"backends/file/","title":"File Backend","text":"<p>The file backend provides local filesystem storage.</p>"},{"location":"backends/file/#installation","title":"Installation","text":"<pre><code>import \"github.com/grokify/omnistorage/backend/file\"\n</code></pre>"},{"location":"backends/file/#usage","title":"Usage","text":""},{"location":"backends/file/#basic-usage","title":"Basic Usage","text":"<pre><code>backend := file.New(file.Config{\n    Root: \"/data\",  // Base directory for all operations\n})\ndefer backend.Close()\n\n// Write\nw, _ := backend.NewWriter(ctx, \"logs/app.log\")\nw.Write([]byte(\"log entry\"))\nw.Close()\n\n// Read\nr, _ := backend.NewReader(ctx, \"logs/app.log\")\ndata, _ := io.ReadAll(r)\nr.Close()\n</code></pre>"},{"location":"backends/file/#using-the-registry","title":"Using the Registry","text":"<pre><code>import (\n    \"github.com/grokify/omnistorage\"\n    _ \"github.com/grokify/omnistorage/backend/file\"\n)\n\nbackend, _ := omnistorage.Open(\"file\", map[string]string{\n    \"root\": \"/data\",\n})\n</code></pre>"},{"location":"backends/file/#configuration","title":"Configuration","text":""},{"location":"backends/file/#config-struct","title":"Config Struct","text":"<pre><code>type Config struct {\n    Root string // Base directory (required)\n}\n</code></pre>"},{"location":"backends/file/#registry-config","title":"Registry Config","text":"Key Description Required <code>root</code> Base directory path Yes"},{"location":"backends/file/#features","title":"Features","text":"<p>The file backend implements <code>ExtendedBackend</code>:</p> Feature Supported Notes Stat Yes Full file metadata Copy Yes Uses <code>os.Link</code> or copy Move Yes Uses <code>os.Rename</code> Mkdir Yes Creates directories Rmdir Yes Removes empty directories"},{"location":"backends/file/#extended-operations","title":"Extended Operations","text":"<pre><code>ext := backend.(*file.Backend)\n\n// Get file metadata\ninfo, _ := ext.Stat(ctx, \"file.txt\")\nfmt.Printf(\"Size: %d bytes\\n\", info.Size())\nfmt.Printf(\"Modified: %s\\n\", info.ModTime())\n\n// Server-side operations\next.Copy(ctx, \"src.txt\", \"dst.txt\")\next.Move(ctx, \"old.txt\", \"new.txt\")\n\n// Directory operations\next.Mkdir(ctx, \"new-folder\")\next.Rmdir(ctx, \"empty-folder\")\n</code></pre>"},{"location":"backends/file/#path-handling","title":"Path Handling","text":"<ul> <li>All paths are relative to the <code>Root</code> directory</li> <li>Parent directories are created automatically on write</li> <li>Path separators are normalized for the OS</li> </ul> <pre><code>backend := file.New(file.Config{Root: \"/data\"})\n\n// This writes to /data/logs/2024/01/app.log\nw, _ := backend.NewWriter(ctx, \"logs/2024/01/app.log\")\n</code></pre>"},{"location":"backends/file/#error-handling","title":"Error Handling","text":"<pre><code>r, err := backend.NewReader(ctx, \"missing.txt\")\nif errors.Is(err, omnistorage.ErrNotFound) {\n    log.Println(\"File not found\")\n}\n</code></pre>"},{"location":"backends/file/#permissions","title":"Permissions","text":"<ul> <li>New files are created with mode <code>0644</code></li> <li>New directories are created with mode <code>0755</code></li> </ul>"},{"location":"backends/file/#best-practices","title":"Best Practices","text":"<ol> <li>Use absolute paths for Root - Relative paths depend on working directory</li> <li>Close writers promptly - Data is flushed on close</li> <li>Handle ErrNotFound - Check for missing files before reading</li> </ol>"},{"location":"backends/memory/","title":"Memory Backend","text":"<p>The memory backend provides in-memory storage, ideal for testing and temporary data.</p>"},{"location":"backends/memory/#installation","title":"Installation","text":"<pre><code>import \"github.com/grokify/omnistorage/backend/memory\"\n</code></pre>"},{"location":"backends/memory/#usage","title":"Usage","text":""},{"location":"backends/memory/#basic-usage","title":"Basic Usage","text":"<pre><code>backend := memory.New()\ndefer backend.Close()\n\n// Write\nw, _ := backend.NewWriter(ctx, \"test/data.json\")\nw.Write([]byte(`{\"key\": \"value\"}`))\nw.Close()\n\n// Read\nr, _ := backend.NewReader(ctx, \"test/data.json\")\ndata, _ := io.ReadAll(r)\nr.Close()\n</code></pre>"},{"location":"backends/memory/#using-the-registry","title":"Using the Registry","text":"<pre><code>import (\n    \"github.com/grokify/omnistorage\"\n    _ \"github.com/grokify/omnistorage/backend/memory\"\n)\n\nbackend, _ := omnistorage.Open(\"memory\", nil)\n</code></pre>"},{"location":"backends/memory/#features","title":"Features","text":"<p>The memory backend implements <code>ExtendedBackend</code>:</p> Feature Supported Notes Stat Yes Full metadata Copy Yes In-memory copy Move Yes Rename + delete Mkdir Yes Virtual directories Rmdir Yes Removes empty directories"},{"location":"backends/memory/#use-cases","title":"Use Cases","text":""},{"location":"backends/memory/#testing","title":"Testing","text":"<pre><code>func TestMyFunction(t *testing.T) {\n    backend := memory.New()\n    defer backend.Close()\n\n    // Use backend in tests\n    err := myFunction(backend)\n    if err != nil {\n        t.Fatal(err)\n    }\n\n    // Verify results\n    r, _ := backend.NewReader(ctx, \"output.txt\")\n    data, _ := io.ReadAll(r)\n    r.Close()\n\n    if string(data) != \"expected\" {\n        t.Errorf(\"got %q, want %q\", data, \"expected\")\n    }\n}\n</code></pre>"},{"location":"backends/memory/#temporary-storage","title":"Temporary Storage","text":"<pre><code>// Use memory backend for intermediate processing\nmem := memory.New()\ndefer mem.Close()\n\n// Process data through memory\nw, _ := mem.NewWriter(ctx, \"temp.json\")\nencoder := json.NewEncoder(w)\nencoder.Encode(data)\nw.Close()\n\n// Read and send elsewhere\nr, _ := mem.NewReader(ctx, \"temp.json\")\nio.Copy(destination, r)\nr.Close()\n</code></pre>"},{"location":"backends/memory/#sync-testing","title":"Sync Testing","text":"<pre><code>// Test sync between memory backends\nsrc := memory.New()\ndst := memory.New()\n\n// Populate source\nw, _ := src.NewWriter(ctx, \"file.txt\")\nw.Write([]byte(\"content\"))\nw.Close()\n\n// Sync\nresult, _ := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{})\n\n// Verify\nexists, _ := dst.Exists(ctx, \"file.txt\")\n// exists == true\n</code></pre>"},{"location":"backends/memory/#extended-operations","title":"Extended Operations","text":"<pre><code>ext := backend.(*memory.Backend)\n\n// Get metadata\ninfo, _ := ext.Stat(ctx, \"file.txt\")\nfmt.Printf(\"Size: %d\\n\", info.Size())\n\n// Copy in memory\next.Copy(ctx, \"src.txt\", \"dst.txt\")\n\n// Move (rename)\next.Move(ctx, \"old.txt\", \"new.txt\")\n</code></pre>"},{"location":"backends/memory/#memory-considerations","title":"Memory Considerations","text":"<ul> <li>Data is stored in memory as <code>[]byte</code> slices</li> <li>No persistence - data is lost when the backend is closed</li> <li>Suitable for testing and temporary data</li> <li>Not suitable for large files or production storage</li> </ul>"},{"location":"backends/memory/#thread-safety","title":"Thread Safety","text":"<p>The memory backend is thread-safe for concurrent read/write operations.</p>"},{"location":"backends/s3/","title":"S3 Backend","text":"<p>The S3 backend supports AWS S3 and S3-compatible storage services including Cloudflare R2, MinIO, Wasabi, DigitalOcean Spaces, and more.</p>"},{"location":"backends/s3/#installation","title":"Installation","text":"<pre><code>import \"github.com/grokify/omnistorage/backend/s3\"\n</code></pre>"},{"location":"backends/s3/#usage","title":"Usage","text":""},{"location":"backends/s3/#aws-s3","title":"AWS S3","text":"<pre><code>backend, err := s3.New(s3.Config{\n    Bucket: \"my-bucket\",\n    Region: \"us-east-1\",\n})\ndefer backend.Close()\n\n// Credentials are loaded from environment or IAM role\n</code></pre>"},{"location":"backends/s3/#cloudflare-r2","title":"Cloudflare R2","text":"<pre><code>backend, err := s3.New(s3.Config{\n    Bucket:   \"my-bucket\",\n    Endpoint: \"https://&lt;account_id&gt;.r2.cloudflarestorage.com\",\n    Region:   \"auto\",\n})\n</code></pre>"},{"location":"backends/s3/#minio-local","title":"MinIO (Local)","text":"<pre><code>backend, err := s3.New(s3.Config{\n    Bucket:       \"my-bucket\",\n    Endpoint:     \"http://localhost:9000\",\n    Region:       \"us-east-1\",\n    UsePathStyle: true,\n    DisableSSL:   true,\n})\n</code></pre>"},{"location":"backends/s3/#from-environment-variables","title":"From Environment Variables","text":"<pre><code>backend, err := s3.New(s3.ConfigFromEnv())\n</code></pre> <p>Environment variables:</p> <ul> <li><code>AWS_REGION</code> or <code>S3_REGION</code></li> <li><code>S3_BUCKET</code></li> <li><code>S3_ENDPOINT</code> (optional, for non-AWS)</li> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> </ul>"},{"location":"backends/s3/#using-the-registry","title":"Using the Registry","text":"<pre><code>import (\n    \"github.com/grokify/omnistorage\"\n    _ \"github.com/grokify/omnistorage/backend/s3\"\n)\n\nbackend, err := omnistorage.Open(\"s3\", map[string]string{\n    \"bucket\":   \"my-bucket\",\n    \"region\":   \"us-east-1\",\n    \"endpoint\": \"\", // Empty for AWS\n})\n</code></pre>"},{"location":"backends/s3/#configuration","title":"Configuration","text":""},{"location":"backends/s3/#config-struct","title":"Config Struct","text":"<pre><code>type Config struct {\n    Bucket       string // Bucket name (required)\n    Region       string // AWS region (required)\n    Endpoint     string // Custom endpoint for R2, MinIO, etc.\n    Prefix       string // Key prefix for all operations\n    UsePathStyle bool   // Use path-style URLs (for MinIO)\n    DisableSSL   bool   // Disable SSL (for local MinIO)\n}\n</code></pre>"},{"location":"backends/s3/#registry-config","title":"Registry Config","text":"Key Description Required <code>bucket</code> S3 bucket name Yes <code>region</code> AWS region Yes <code>endpoint</code> Custom endpoint URL No <code>prefix</code> Key prefix No <code>use_path_style</code> Use path-style URLs No <code>disable_ssl</code> Disable SSL No"},{"location":"backends/s3/#features","title":"Features","text":"<p>The S3 backend implements <code>ExtendedBackend</code>:</p> Feature Supported Notes Stat Yes Uses HeadObject Copy Yes Server-side CopyObject Move Yes Copy + Delete Mkdir Yes Creates empty prefix Rmdir Yes Deletes prefix"},{"location":"backends/s3/#operations","title":"Operations","text":""},{"location":"backends/s3/#write","title":"Write","text":"<pre><code>w, err := backend.NewWriter(ctx, \"data/file.json\")\nif err != nil {\n    return err\n}\nw.Write([]byte(`{\"key\": \"value\"}`))\nw.Close() // Upload happens on close\n</code></pre>"},{"location":"backends/s3/#read","title":"Read","text":"<pre><code>r, err := backend.NewReader(ctx, \"data/file.json\")\nif err != nil {\n    return err\n}\ndefer r.Close()\ndata, _ := io.ReadAll(r)\n</code></pre>"},{"location":"backends/s3/#list","title":"List","text":"<pre><code>// List all files with prefix\nfiles, err := backend.List(ctx, \"data/\")\nfor _, f := range files {\n    fmt.Println(f)\n}\n</code></pre>"},{"location":"backends/s3/#extended-operations","title":"Extended Operations","text":"<pre><code>ext := backend.(*s3.Backend)\n\n// Get object metadata\ninfo, _ := ext.Stat(ctx, \"file.txt\")\nfmt.Printf(\"Size: %d bytes\\n\", info.Size())\nfmt.Printf(\"ETag: %s\\n\", info.Hash(omnistorage.HashMD5))\n\n// Server-side copy (efficient, no download)\next.Copy(ctx, \"source.txt\", \"dest.txt\")\n\n// Server-side move\next.Move(ctx, \"old.txt\", \"new.txt\")\n</code></pre>"},{"location":"backends/s3/#multipart-uploads","title":"Multipart Uploads","text":"<p>Large files are automatically uploaded using multipart uploads via the AWS SDK's upload manager.</p>"},{"location":"backends/s3/#content-types","title":"Content Types","text":"<p>Set content type on upload:</p> <pre><code>w, _ := backend.NewWriter(ctx, \"data.json\",\n    omnistorage.WithContentType(\"application/json\"))\n</code></pre>"},{"location":"backends/s3/#error-handling","title":"Error Handling","text":"<pre><code>r, err := backend.NewReader(ctx, \"missing.txt\")\nif errors.Is(err, omnistorage.ErrNotFound) {\n    log.Println(\"Object not found\")\n}\n</code></pre>"},{"location":"backends/s3/#best-practices","title":"Best Practices","text":"<ol> <li>Use regions close to your application - Reduces latency</li> <li>Use server-side copy - Check <code>Features().Copy</code> before copying</li> <li>Stream large files - Don't load entire files into memory</li> <li>Close writers - Uploads complete on <code>Close()</code></li> </ol>"},{"location":"backends/sftp/","title":"SFTP Backend","text":"<p>The SFTP backend provides access to remote servers via SSH File Transfer Protocol (SFTP). It supports both password and SSH key authentication.</p>"},{"location":"backends/sftp/#installation","title":"Installation","text":"<pre><code>import \"github.com/grokify/omnistorage/backend/sftp\"\n</code></pre>"},{"location":"backends/sftp/#usage","title":"Usage","text":""},{"location":"backends/sftp/#password-authentication","title":"Password Authentication","text":"<pre><code>backend, err := sftp.New(sftp.Config{\n    Host:     \"example.com\",\n    User:     \"username\",\n    Password: \"password\",\n})\ndefer backend.Close()\n</code></pre>"},{"location":"backends/sftp/#ssh-key-authentication","title":"SSH Key Authentication","text":"<pre><code>backend, err := sftp.New(sftp.Config{\n    Host:    \"example.com\",\n    User:    \"username\",\n    KeyFile: \"/path/to/id_rsa\",\n})\n</code></pre>"},{"location":"backends/sftp/#with-encrypted-key","title":"With Encrypted Key","text":"<pre><code>backend, err := sftp.New(sftp.Config{\n    Host:          \"example.com\",\n    User:          \"username\",\n    KeyFile:       \"/path/to/id_rsa\",\n    KeyPassphrase: \"keypassword\",\n})\n</code></pre>"},{"location":"backends/sftp/#from-environment-variables","title":"From Environment Variables","text":"<pre><code>backend, err := sftp.New(sftp.ConfigFromEnv())\n</code></pre> <p>Environment variables:</p> <ul> <li><code>OMNISTORAGE_SFTP_HOST</code> - Server hostname</li> <li><code>OMNISTORAGE_SFTP_PORT</code> - SSH port (default: 22)</li> <li><code>OMNISTORAGE_SFTP_USER</code> - Username</li> <li><code>OMNISTORAGE_SFTP_PASSWORD</code> - Password</li> <li><code>OMNISTORAGE_SFTP_KEY_FILE</code> - Path to private key</li> <li><code>OMNISTORAGE_SFTP_KEY_PASSPHRASE</code> - Key passphrase</li> <li><code>OMNISTORAGE_SFTP_ROOT</code> - Base directory</li> <li><code>OMNISTORAGE_SFTP_KNOWN_HOSTS</code> - Path to known_hosts file</li> <li><code>OMNISTORAGE_SFTP_TIMEOUT</code> - Connection timeout in seconds</li> </ul>"},{"location":"backends/sftp/#using-the-registry","title":"Using the Registry","text":"<pre><code>import (\n    \"github.com/grokify/omnistorage\"\n    _ \"github.com/grokify/omnistorage/backend/sftp\"\n)\n\nbackend, err := omnistorage.Open(\"sftp\", map[string]string{\n    \"host\":     \"example.com\",\n    \"user\":     \"username\",\n    \"password\": \"password\",\n    \"root\":     \"/data\",\n})\n</code></pre>"},{"location":"backends/sftp/#configuration","title":"Configuration","text":""},{"location":"backends/sftp/#config-struct","title":"Config Struct","text":"<pre><code>type Config struct {\n    Host           string // Server hostname (required)\n    Port           int    // SSH port (default: 22)\n    User           string // Username (required)\n    Password       string // Password auth\n    KeyFile        string // Path to private key\n    KeyPassphrase  string // Passphrase for encrypted keys\n    Root           string // Base directory for operations\n    KnownHostsFile string // Path to known_hosts file\n    Timeout        int    // Connection timeout in seconds (default: 30)\n    Concurrency    int    // Max concurrent operations (default: 5)\n}\n</code></pre>"},{"location":"backends/sftp/#registry-config","title":"Registry Config","text":"Key Description Required <code>host</code> Server hostname Yes <code>port</code> SSH port No (default: 22) <code>user</code> Username Yes <code>password</code> Password No* <code>key_file</code> Path to private key No* <code>key_passphrase</code> Key passphrase No <code>root</code> Base directory No <code>known_hosts</code> Path to known_hosts file No <code>timeout</code> Timeout in seconds No <p>* Either <code>password</code> or <code>key_file</code> is required.</p>"},{"location":"backends/sftp/#features","title":"Features","text":"<p>The SFTP backend implements <code>ExtendedBackend</code>:</p> Feature Supported Notes Stat Yes Returns file metadata Copy Yes Client-side streaming copy Move Yes Uses rename or copy+delete Mkdir Yes Creates directories recursively Rmdir Yes Removes empty directories Range Read Yes Offset and limit supported"},{"location":"backends/sftp/#operations","title":"Operations","text":""},{"location":"backends/sftp/#write","title":"Write","text":"<pre><code>w, err := backend.NewWriter(ctx, \"data/file.json\")\nif err != nil {\n    return err\n}\nw.Write([]byte(`{\"key\": \"value\"}`))\nw.Close()\n</code></pre>"},{"location":"backends/sftp/#read","title":"Read","text":"<pre><code>r, err := backend.NewReader(ctx, \"data/file.json\")\nif err != nil {\n    return err\n}\ndefer r.Close()\ndata, _ := io.ReadAll(r)\n</code></pre>"},{"location":"backends/sftp/#range-read","title":"Range Read","text":"<pre><code>r, err := backend.NewReader(ctx, \"large-file.bin\",\n    omnistorage.WithOffset(1000),\n    omnistorage.WithLimit(500))\n</code></pre>"},{"location":"backends/sftp/#list","title":"List","text":"<pre><code>files, err := backend.List(ctx, \"data/\")\nfor _, f := range files {\n    fmt.Println(f)\n}\n</code></pre>"},{"location":"backends/sftp/#extended-operations","title":"Extended Operations","text":"<pre><code>ext := backend.(*sftp.Backend)\n\n// Get file metadata\ninfo, _ := ext.Stat(ctx, \"file.txt\")\nfmt.Printf(\"Size: %d bytes\\n\", info.Size())\nfmt.Printf(\"Modified: %s\\n\", info.ModTime())\n\n// Create directory\next.Mkdir(ctx, \"new-folder\")\n\n// Copy file\next.Copy(ctx, \"source.txt\", \"dest.txt\")\n\n// Move file (uses rename if on same filesystem)\next.Move(ctx, \"old.txt\", \"new.txt\")\n</code></pre>"},{"location":"backends/sftp/#authentication","title":"Authentication","text":""},{"location":"backends/sftp/#password-vs-key-authentication","title":"Password vs Key Authentication","text":"<p>Password authentication is simpler but less secure. SSH key authentication is recommended for production:</p> <pre><code>// Production: Use SSH key\nbackend, _ := sftp.New(sftp.Config{\n    Host:    \"prod.example.com\",\n    User:    \"deploy\",\n    KeyFile: \"/home/app/.ssh/id_ed25519\",\n})\n</code></pre>"},{"location":"backends/sftp/#host-key-verification","title":"Host Key Verification","text":"<p>By default, host key verification is disabled for development convenience. For production, specify a known_hosts file:</p> <pre><code>backend, _ := sftp.New(sftp.Config{\n    Host:           \"prod.example.com\",\n    User:           \"deploy\",\n    KeyFile:        \"/home/app/.ssh/id_ed25519\",\n    KnownHostsFile: \"/home/app/.ssh/known_hosts\",\n})\n</code></pre>"},{"location":"backends/sftp/#error-handling","title":"Error Handling","text":"<pre><code>r, err := backend.NewReader(ctx, \"missing.txt\")\nif errors.Is(err, omnistorage.ErrNotFound) {\n    log.Println(\"File not found\")\n}\n\nif errors.Is(err, omnistorage.ErrPermissionDenied) {\n    log.Println(\"Permission denied\")\n}\n</code></pre>"},{"location":"backends/sftp/#best-practices","title":"Best Practices","text":"<ol> <li>Use SSH keys - More secure than passwords</li> <li>Set a root directory - Avoid path traversal issues</li> <li>Enable host key verification - Required for production</li> <li>Handle connection errors - Network issues are common</li> <li>Close the backend - Releases SSH connection</li> </ol>"},{"location":"getting-started/concepts/","title":"Concepts","text":"<p>This page explains the core concepts and architecture of omnistorage.</p>"},{"location":"getting-started/concepts/#layered-architecture","title":"Layered Architecture","text":"<p>Omnistorage uses a layered architecture where each layer handles a specific concern:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Application Layer                         \u2502\n\u2502         (Your code: marshals domain types, logs, etc.)      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                     Format Layer                             \u2502\n\u2502              (NDJSON, length-prefixed, CSV)                 \u2502\n\u2502              Handles record framing/delimiting              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                   Compression Layer                          \u2502\n\u2502                  (gzip, zstd, snappy)                       \u2502\n\u2502              Optional compression/decompression             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    Backend Layer                             \u2502\n\u2502        (File, S3, GCS, Channel, Memory, etc.)               \u2502\n\u2502              Raw byte transport/storage                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Each layer is independent and composable. You can mix and match:</p> <ul> <li>File backend + gzip compression + NDJSON format</li> <li>S3 backend + zstd compression + raw bytes</li> <li>Memory backend + no compression + CSV format</li> </ul>"},{"location":"getting-started/concepts/#interface-composition","title":"Interface Composition","text":"<p>Omnistorage uses interface composition to support both simple and advanced use cases:</p>"},{"location":"getting-started/concepts/#backend-basic-interface","title":"Backend (Basic Interface)","text":"<p>The <code>Backend</code> interface provides core read/write operations:</p> <pre><code>type Backend interface {\n    NewWriter(ctx context.Context, path string, opts ...WriterOption) (io.WriteCloser, error)\n    NewReader(ctx context.Context, path string, opts ...ReaderOption) (io.ReadCloser, error)\n    Exists(ctx context.Context, path string) (bool, error)\n    Delete(ctx context.Context, path string) error\n    List(ctx context.Context, prefix string) ([]string, error)\n    Close() error\n}\n</code></pre> <p>This is sufficient for most applications.</p>"},{"location":"getting-started/concepts/#extendedbackend-advanced-interface","title":"ExtendedBackend (Advanced Interface)","text":"<p>The <code>ExtendedBackend</code> interface adds metadata and server-side operations:</p> <pre><code>type ExtendedBackend interface {\n    Backend\n    Stat(ctx context.Context, path string) (ObjectInfo, error)\n    Mkdir(ctx context.Context, path string) error\n    Rmdir(ctx context.Context, path string) error\n    Copy(ctx context.Context, src, dst string) error\n    Move(ctx context.Context, src, dst string) error\n    Features() Features\n}\n</code></pre> <p>Use <code>AsExtended()</code> to check if a backend supports extended operations:</p> <pre><code>if ext, ok := omnistorage.AsExtended(backend); ok {\n    info, _ := ext.Stat(ctx, \"file.txt\")\n    fmt.Printf(\"Size: %d bytes\\n\", info.Size())\n}\n</code></pre>"},{"location":"getting-started/concepts/#backend-registration","title":"Backend Registration","text":"<p>Backends register themselves using the <code>Register()</code> function, typically in <code>init()</code>:</p> <pre><code>// backend/file/backend.go\nfunc init() {\n    omnistorage.Register(\"file\", NewFromConfig)\n}\n</code></pre> <p>This allows configuration-driven backend selection:</p> <pre><code>// Select backend from environment variable\nbackendType := os.Getenv(\"STORAGE_BACKEND\") // \"file\", \"s3\", etc.\nbackend, _ := omnistorage.Open(backendType, config)\n</code></pre>"},{"location":"getting-started/concepts/#feature-discovery","title":"Feature Discovery","text":"<p>Backends advertise their capabilities through the <code>Features</code> struct:</p> <pre><code>type Features struct {\n    Copy           bool // Server-side copy\n    Move           bool // Server-side move\n    Purge          bool // Recursive delete\n    SetModTime     bool // Set modification time\n    CustomMetadata bool // Custom metadata support\n}\n</code></pre> <p>This allows code to adapt to backend capabilities:</p> <pre><code>if ext.Features().Copy {\n    // Use efficient server-side copy\n    ext.Copy(ctx, src, dst)\n} else {\n    // Fall back to read + write\n    omnistorage.CopyPath(ctx, backend, src, backend, dst)\n}\n</code></pre>"},{"location":"getting-started/concepts/#objectinfo","title":"ObjectInfo","text":"<p>The <code>ObjectInfo</code> interface provides file metadata:</p> <pre><code>type ObjectInfo interface {\n    Name() string\n    Size() int64\n    ModTime() time.Time\n    IsDir() bool\n    Hash(HashType) string\n    MimeType() string\n    Metadata() map[string]string\n}\n</code></pre>"},{"location":"getting-started/concepts/#recordwriter-recordreader","title":"RecordWriter / RecordReader","text":"<p>For streaming record-oriented data (logs, events, NDJSON):</p> <pre><code>type RecordWriter interface {\n    Write(data []byte) error  // Write a single record\n    Flush() error             // Flush buffered data\n    Close() error\n}\n\ntype RecordReader interface {\n    Read() ([]byte, error)    // Read next record (io.EOF when done)\n    Close() error\n}\n</code></pre>"},{"location":"getting-started/concepts/#sync-engine","title":"Sync Engine","text":"<p>The sync package provides rclone-like file synchronization:</p> <ul> <li>Sync - Make destination match source (with optional deletes)</li> <li>Copy - Copy files without deleting extras</li> <li>Move - Move files from source to destination</li> <li>Check - Verify files match between backends</li> </ul> <p>See Sync Engine for details.</p>"},{"location":"getting-started/concepts/#multi-writer","title":"Multi-Writer","text":"<p>The multi package provides fan-out writing to multiple backends:</p> <pre><code>mw, _ := multi.NewWriter(local, s3, gcs)\nw, _ := mw.NewWriter(ctx, \"data.json\")\nw.Write(data)  // Written to all three backends\nw.Close()\n</code></pre> <p>See Multi-Writer Guide for details.</p>"},{"location":"getting-started/concepts/#error-handling","title":"Error Handling","text":"<p>Omnistorage defines standard errors for common cases:</p> <pre><code>var (\n    ErrNotFound        // Path does not exist\n    ErrAlreadyExists   // Path already exists\n    ErrPermissionDenied // Access denied\n    ErrBackendClosed   // Backend has been closed\n    ErrInvalidPath     // Invalid path format\n    ErrWriterClosed    // Writer has been closed\n    ErrReaderClosed    // Reader has been closed\n)\n</code></pre> <p>Use <code>errors.Is()</code> to check:</p> <pre><code>r, err := backend.NewReader(ctx, \"missing.txt\")\nif errors.Is(err, omnistorage.ErrNotFound) {\n    log.Println(\"File not found\")\n}\n</code></pre>"},{"location":"getting-started/concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Backends - Learn about specific backends</li> <li>Sync Engine - File synchronization</li> <li>Reference - Complete API reference</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Go 1.21 or later</li> </ul>"},{"location":"getting-started/installation/#install","title":"Install","text":"<pre><code>go get github.com/grokify/omnistorage\n</code></pre>"},{"location":"getting-started/installation/#backend-specific-dependencies","title":"Backend-Specific Dependencies","text":"<p>The core package has minimal dependencies. Backend-specific packages bring in their own dependencies:</p>"},{"location":"getting-started/installation/#s3-backend","title":"S3 Backend","text":"<pre><code>go get github.com/grokify/omnistorage/backend/s3\n</code></pre> <p>This brings in the AWS SDK v2.</p>"},{"location":"getting-started/installation/#google-drive-backend","title":"Google Drive Backend","text":"<p>Google backends are in a separate repository to keep the core lightweight:</p> <pre><code>go get github.com/grokify/omnistorage-google\n</code></pre>"},{"location":"getting-started/installation/#zstandard-compression","title":"Zstandard Compression","text":"<pre><code>go get github.com/grokify/omnistorage/compress/zstd\n</code></pre> <p>This brings in <code>github.com/klauspost/compress</code>.</p>"},{"location":"getting-started/installation/#import-patterns","title":"Import Patterns","text":""},{"location":"getting-started/installation/#direct-backend-usage","title":"Direct Backend Usage","text":"<pre><code>import (\n    \"github.com/grokify/omnistorage/backend/file\"\n    \"github.com/grokify/omnistorage/backend/s3\"\n)\n\n// Use backends directly\nfileBackend := file.New(file.Config{Root: \"/data\"})\ns3Backend, _ := s3.New(s3.Config{Bucket: \"my-bucket\"})\n</code></pre>"},{"location":"getting-started/installation/#registry-pattern","title":"Registry Pattern","text":"<pre><code>import (\n    \"github.com/grokify/omnistorage\"\n\n    // Side-effect imports register backends\n    _ \"github.com/grokify/omnistorage/backend/file\"\n    _ \"github.com/grokify/omnistorage/backend/s3\"\n)\n\n// Open by name from configuration\nbackend, _ := omnistorage.Open(\"s3\", map[string]string{\n    \"bucket\": \"my-bucket\",\n    \"region\": \"us-east-1\",\n})\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n    \"github.com/grokify/omnistorage\"\n    _ \"github.com/grokify/omnistorage/backend/file\"\n    _ \"github.com/grokify/omnistorage/backend/memory\"\n)\n\nfunc main() {\n    backends := omnistorage.Backends()\n    fmt.Println(\"Registered backends:\", backends)\n    // Output: Registered backends: [file memory]\n}\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Learn the basics</li> <li>Concepts - Understand the architecture</li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>This guide covers the most common omnistorage operations.</p>"},{"location":"getting-started/quick-start/#basic-readwrite","title":"Basic Read/Write","text":"<pre><code>package main\n\nimport (\n    \"context\"\n    \"io\"\n    \"log\"\n\n    \"github.com/grokify/omnistorage/backend/file\"\n)\n\nfunc main() {\n    ctx := context.Background()\n\n    // Create a file backend\n    backend := file.New(file.Config{Root: \"/data\"})\n    defer backend.Close()\n\n    // Write a file\n    w, err := backend.NewWriter(ctx, \"hello.txt\")\n    if err != nil {\n        log.Fatal(err)\n    }\n    w.Write([]byte(\"Hello, World!\"))\n    w.Close()\n\n    // Read it back\n    r, err := backend.NewReader(ctx, \"hello.txt\")\n    if err != nil {\n        log.Fatal(err)\n    }\n    data, _ := io.ReadAll(r)\n    r.Close()\n\n    log.Println(string(data)) // \"Hello, World!\"\n}\n</code></pre>"},{"location":"getting-started/quick-start/#with-compression","title":"With Compression","text":"<pre><code>import (\n    \"github.com/grokify/omnistorage/backend/file\"\n    \"github.com/grokify/omnistorage/compress/gzip\"\n)\n\n// Write compressed data\nfileWriter, _ := backend.NewWriter(ctx, \"data.txt.gz\")\ngzipWriter, _ := gzip.NewWriter(fileWriter)\ngzipWriter.Write([]byte(\"compressed content\"))\ngzipWriter.Close()\n\n// Read compressed data\nfileReader, _ := backend.NewReader(ctx, \"data.txt.gz\")\ngzipReader, _ := gzip.NewReader(fileReader)\ndata, _ := io.ReadAll(gzipReader)\ngzipReader.Close()\n</code></pre>"},{"location":"getting-started/quick-start/#with-ndjson-records","title":"With NDJSON Records","text":"<pre><code>import (\n    \"github.com/grokify/omnistorage/backend/file\"\n    \"github.com/grokify/omnistorage/format/ndjson\"\n)\n\n// Write NDJSON records\nw, _ := backend.NewWriter(ctx, \"records.ndjson\")\nndjsonWriter := ndjson.NewWriter(w)\nndjsonWriter.Write([]byte(`{\"id\":1,\"name\":\"alice\"}`))\nndjsonWriter.Write([]byte(`{\"id\":2,\"name\":\"bob\"}`))\nndjsonWriter.Close()\n\n// Read NDJSON records\nr, _ := backend.NewReader(ctx, \"records.ndjson\")\nndjsonReader := ndjson.NewReader(r)\nfor {\n    record, err := ndjsonReader.Read()\n    if err == io.EOF {\n        break\n    }\n    log.Println(string(record))\n}\nndjsonReader.Close()\n</code></pre>"},{"location":"getting-started/quick-start/#using-the-registry","title":"Using the Registry","text":"<pre><code>import (\n    \"github.com/grokify/omnistorage\"\n    _ \"github.com/grokify/omnistorage/backend/file\"\n    _ \"github.com/grokify/omnistorage/backend/s3\"\n)\n\n// Open backend by name\nbackend, _ := omnistorage.Open(\"file\", map[string]string{\n    \"root\": \"/data\",\n})\ndefer backend.Close()\n\n// List registered backends\nbackends := omnistorage.Backends() // [\"file\", \"memory\", \"s3\"]\n</code></pre>"},{"location":"getting-started/quick-start/#sync-between-backends","title":"Sync Between Backends","text":"<pre><code>import \"github.com/grokify/omnistorage/sync\"\n\nsrcBackend := file.New(file.Config{Root: \"/local\"})\ndstBackend, _ := s3.New(s3.Config{Bucket: \"my-bucket\"})\n\n// Sync local to S3\nresult, err := sync.Sync(ctx, srcBackend, dstBackend, \"data/\", \"backup/\", sync.Options{\n    DeleteExtra: true,  // Delete files in dst not in src\n})\n\nfmt.Printf(\"Copied: %d, Updated: %d, Deleted: %d\\n\",\n    result.Copied, result.Updated, result.Deleted)\n</code></pre>"},{"location":"getting-started/quick-start/#check-file-existence","title":"Check File Existence","text":"<pre><code>exists, err := backend.Exists(ctx, \"hello.txt\")\nif err != nil {\n    log.Fatal(err)\n}\nif exists {\n    log.Println(\"File exists\")\n}\n</code></pre>"},{"location":"getting-started/quick-start/#list-files","title":"List Files","text":"<pre><code>// List all files with prefix\nfiles, err := backend.List(ctx, \"logs/\")\nif err != nil {\n    log.Fatal(err)\n}\nfor _, f := range files {\n    log.Println(f)\n}\n</code></pre>"},{"location":"getting-started/quick-start/#delete-files","title":"Delete Files","text":"<pre><code>err := backend.Delete(ctx, \"hello.txt\")\nif err != nil {\n    log.Fatal(err)\n}\n</code></pre>"},{"location":"getting-started/quick-start/#extended-backend-features","title":"Extended Backend Features","text":"<pre><code>// Check if backend supports extended operations\nif ext, ok := omnistorage.AsExtended(backend); ok {\n    // Get file metadata\n    info, _ := ext.Stat(ctx, \"file.txt\")\n    fmt.Printf(\"Size: %d, Modified: %s\\n\", info.Size(), info.ModTime())\n\n    // Server-side copy (no download/upload)\n    if ext.Features().Copy {\n        ext.Copy(ctx, \"source.txt\", \"dest.txt\")\n    }\n\n    // Directory operations\n    ext.Mkdir(ctx, \"new-folder\")\n}\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Concepts - Understand the architecture</li> <li>Backends - Learn about specific backends</li> <li>Sync Engine - Advanced sync operations</li> </ul>"},{"location":"guides/compression/","title":"Compression Guide","text":"<p>Omnistorage provides compression layers that wrap io.Writer and io.Reader.</p>"},{"location":"guides/compression/#available-compressors","title":"Available Compressors","text":"Format Package Use Case Gzip <code>compress/gzip</code> Universal compatibility Zstandard <code>compress/zstd</code> Better compression ratio and speed"},{"location":"guides/compression/#gzip-compression","title":"Gzip Compression","text":""},{"location":"guides/compression/#writing-compressed-data","title":"Writing Compressed Data","text":"<pre><code>import (\n    \"github.com/grokify/omnistorage/backend/file\"\n    \"github.com/grokify/omnistorage/compress/gzip\"\n)\n\nbackend := file.New(file.Config{Root: \"/data\"})\n\n// Create the writer stack\nfileWriter, _ := backend.NewWriter(ctx, \"data.txt.gz\")\ngzipWriter, _ := gzip.NewWriter(fileWriter)\n\n// Write data\ngzipWriter.Write([]byte(\"compressed content\"))\ngzipWriter.Close() // Important: closes both gzip and file writers\n</code></pre>"},{"location":"guides/compression/#reading-compressed-data","title":"Reading Compressed Data","text":"<pre><code>fileReader, _ := backend.NewReader(ctx, \"data.txt.gz\")\ngzipReader, _ := gzip.NewReader(fileReader)\ndefer gzipReader.Close()\n\ndata, _ := io.ReadAll(gzipReader)\n</code></pre>"},{"location":"guides/compression/#compression-level","title":"Compression Level","text":"<pre><code>// Default compression\ngzipWriter, _ := gzip.NewWriter(fileWriter)\n\n// Best compression (slower)\ngzipWriter, _ := gzip.NewWriterLevel(fileWriter, gzip.BestCompression)\n\n// Best speed (larger files)\ngzipWriter, _ := gzip.NewWriterLevel(fileWriter, gzip.BestSpeed)\n\n// No compression (for testing)\ngzipWriter, _ := gzip.NewWriterLevel(fileWriter, gzip.NoCompression)\n</code></pre>"},{"location":"guides/compression/#zstandard-compression","title":"Zstandard Compression","text":"<p>Zstandard (zstd) provides better compression ratio and faster decompression than gzip.</p>"},{"location":"guides/compression/#writing-with-zstd","title":"Writing with Zstd","text":"<pre><code>import \"github.com/grokify/omnistorage/compress/zstd\"\n\nfileWriter, _ := backend.NewWriter(ctx, \"data.txt.zst\")\nzstdWriter, _ := zstd.NewWriter(fileWriter)\n\nzstdWriter.Write([]byte(\"compressed content\"))\nzstdWriter.Close()\n</code></pre>"},{"location":"guides/compression/#reading-with-zstd","title":"Reading with Zstd","text":"<pre><code>fileReader, _ := backend.NewReader(ctx, \"data.txt.zst\")\nzstdReader, _ := zstd.NewReader(fileReader)\ndefer zstdReader.Close()\n\ndata, _ := io.ReadAll(zstdReader)\n</code></pre>"},{"location":"guides/compression/#compression-level_1","title":"Compression Level","text":"<pre><code>// Default level\nzstdWriter, _ := zstd.NewWriter(fileWriter)\n\n// Custom level (1-22, default is 3)\nzstdWriter, _ := zstd.NewWriterLevel(fileWriter, 10)\n</code></pre>"},{"location":"guides/compression/#combining-with-format-layers","title":"Combining with Format Layers","text":"<p>Stack compression with format layers:</p> <pre><code>import (\n    \"github.com/grokify/omnistorage/backend/s3\"\n    \"github.com/grokify/omnistorage/compress/gzip\"\n    \"github.com/grokify/omnistorage/format/ndjson\"\n)\n\ns3Backend, _ := s3.New(s3Config)\n\n// Create writer stack: S3 -&gt; Gzip -&gt; NDJSON\nraw, _ := s3Backend.NewWriter(ctx, \"logs/2024-01-08.ndjson.gz\")\ncompressed := gzip.NewWriter(raw)\nwriter := ndjson.NewWriter(compressed)\n\n// Write records\nfor _, record := range records {\n    data, _ := json.Marshal(record)\n    writer.Write(data)\n}\nwriter.Close() // Closes entire stack\n</code></pre>"},{"location":"guides/compression/#reading-the-stack","title":"Reading the Stack","text":"<pre><code>raw, _ := s3Backend.NewReader(ctx, \"logs/2024-01-08.ndjson.gz\")\ndecompressed, _ := gzip.NewReader(raw)\nreader := ndjson.NewReader(decompressed)\n\nfor {\n    record, err := reader.Read()\n    if err == io.EOF {\n        break\n    }\n    process(record)\n}\nreader.Close()\n</code></pre>"},{"location":"guides/compression/#choosing-a-compressor","title":"Choosing a Compressor","text":"Factor Gzip Zstd Compatibility Universal Growing Compression ratio Good Better Compression speed Moderate Fast Decompression speed Moderate Very fast Memory usage Low Low-Medium"},{"location":"guides/compression/#when-to-use-gzip","title":"When to Use Gzip","text":"<ul> <li>Compatibility is important (web servers, browsers)</li> <li>Files will be served over HTTP</li> <li>Working with legacy systems</li> </ul>"},{"location":"guides/compression/#when-to-use-zstd","title":"When to Use Zstd","text":"<ul> <li>Better compression is important</li> <li>Fast decompression is needed</li> <li>Processing large data volumes</li> <li>Internal/controlled environments</li> </ul>"},{"location":"guides/compression/#file-extensions","title":"File Extensions","text":"<p>Follow conventions for file extensions:</p> Format Extension Gzip <code>.gz</code> Zstd <code>.zst</code> or <code>.zstd</code> <p>Combine with format extensions:</p> <ul> <li><code>data.json.gz</code> - Gzip-compressed JSON</li> <li><code>logs.ndjson.zst</code> - Zstd-compressed NDJSON</li> </ul>"},{"location":"guides/compression/#error-handling","title":"Error Handling","text":"<pre><code>gzipReader, err := gzip.NewReader(fileReader)\nif err != nil {\n    // Invalid gzip header or corrupted data\n    return fmt.Errorf(\"failed to create gzip reader: %w\", err)\n}\n</code></pre>"},{"location":"guides/compression/#best-practices","title":"Best Practices","text":"<ol> <li>Close writers in reverse order - Or just close the outermost writer</li> <li>Use appropriate compression level - Balance speed vs size</li> <li>Follow naming conventions - Use <code>.gz</code> or <code>.zst</code> extensions</li> <li>Stream large files - Don't load entire files into memory</li> </ol>"},{"location":"guides/custom-backend/","title":"Custom Backend Guide","text":"<p>This guide shows how to implement a custom omnistorage backend.</p>"},{"location":"guides/custom-backend/#overview","title":"Overview","text":"<p>A backend implements the <code>Backend</code> interface:</p> <pre><code>type Backend interface {\n    NewWriter(ctx context.Context, path string, opts ...WriterOption) (io.WriteCloser, error)\n    NewReader(ctx context.Context, path string, opts ...ReaderOption) (io.ReadCloser, error)\n    Exists(ctx context.Context, path string) (bool, error)\n    Delete(ctx context.Context, path string) error\n    List(ctx context.Context, prefix string) ([]string, error)\n    Close() error\n}\n</code></pre>"},{"location":"guides/custom-backend/#basic-implementation","title":"Basic Implementation","text":""},{"location":"guides/custom-backend/#step-1-create-the-package","title":"Step 1: Create the Package","text":"<pre><code>// backend/mycloud/backend.go\npackage mycloud\n\nimport (\n    \"context\"\n    \"io\"\n\n    \"github.com/grokify/omnistorage\"\n)\n</code></pre>"},{"location":"guides/custom-backend/#step-2-define-the-backend-struct","title":"Step 2: Define the Backend Struct","text":"<pre><code>type Backend struct {\n    client *MyCloudClient\n    bucket string\n    closed bool\n    mu     sync.RWMutex\n}\n\ntype Config struct {\n    Bucket   string\n    APIKey   string\n    Endpoint string\n}\n\nfunc New(config Config) (*Backend, error) {\n    client, err := NewMyCloudClient(config.APIKey, config.Endpoint)\n    if err != nil {\n        return nil, err\n    }\n\n    return &amp;Backend{\n        client: client,\n        bucket: config.Bucket,\n    }, nil\n}\n</code></pre>"},{"location":"guides/custom-backend/#step-3-implement-newwriter","title":"Step 3: Implement NewWriter","text":"<pre><code>func (b *Backend) NewWriter(ctx context.Context, path string, opts ...omnistorage.WriterOption) (io.WriteCloser, error) {\n    if err := b.checkClosed(); err != nil {\n        return nil, err\n    }\n\n    if err := ctx.Err(); err != nil {\n        return nil, err\n    }\n\n    if path == \"\" {\n        return nil, omnistorage.ErrInvalidPath\n    }\n\n    // Apply options\n    config := omnistorage.DefaultWriterConfig()\n    for _, opt := range opts {\n        opt(&amp;config)\n    }\n\n    return &amp;writer{\n        backend:     b,\n        path:        path,\n        contentType: config.ContentType,\n        buffer:      &amp;bytes.Buffer{},\n    }, nil\n}\n\ntype writer struct {\n    backend     *Backend\n    path        string\n    contentType string\n    buffer      *bytes.Buffer\n    closed      bool\n}\n\nfunc (w *writer) Write(p []byte) (n int, err error) {\n    if w.closed {\n        return 0, omnistorage.ErrWriterClosed\n    }\n    return w.buffer.Write(p)\n}\n\nfunc (w *writer) Close() error {\n    if w.closed {\n        return nil\n    }\n    w.closed = true\n\n    // Upload buffered data to cloud\n    return w.backend.client.Upload(w.path, w.buffer.Bytes(), w.contentType)\n}\n</code></pre>"},{"location":"guides/custom-backend/#step-4-implement-newreader","title":"Step 4: Implement NewReader","text":"<pre><code>func (b *Backend) NewReader(ctx context.Context, path string, opts ...omnistorage.ReaderOption) (io.ReadCloser, error) {\n    if err := b.checkClosed(); err != nil {\n        return nil, err\n    }\n\n    if err := ctx.Err(); err != nil {\n        return nil, err\n    }\n\n    if path == \"\" {\n        return nil, omnistorage.ErrInvalidPath\n    }\n\n    // Download from cloud\n    data, err := b.client.Download(path)\n    if err != nil {\n        if isNotFoundError(err) {\n            return nil, omnistorage.ErrNotFound\n        }\n        return nil, err\n    }\n\n    return io.NopCloser(bytes.NewReader(data)), nil\n}\n</code></pre>"},{"location":"guides/custom-backend/#step-5-implement-other-methods","title":"Step 5: Implement Other Methods","text":"<pre><code>func (b *Backend) Exists(ctx context.Context, path string) (bool, error) {\n    if err := b.checkClosed(); err != nil {\n        return false, err\n    }\n\n    exists, err := b.client.Exists(path)\n    if err != nil {\n        return false, err\n    }\n    return exists, nil\n}\n\nfunc (b *Backend) Delete(ctx context.Context, path string) error {\n    if err := b.checkClosed(); err != nil {\n        return err\n    }\n\n    err := b.client.Delete(path)\n    if isNotFoundError(err) {\n        return nil // Idempotent\n    }\n    return err\n}\n\nfunc (b *Backend) List(ctx context.Context, prefix string) ([]string, error) {\n    if err := b.checkClosed(); err != nil {\n        return nil, err\n    }\n\n    return b.client.List(prefix)\n}\n\nfunc (b *Backend) Close() error {\n    b.mu.Lock()\n    defer b.mu.Unlock()\n\n    if b.closed {\n        return nil\n    }\n    b.closed = true\n\n    return b.client.Close()\n}\n\nfunc (b *Backend) checkClosed() error {\n    b.mu.RLock()\n    defer b.mu.RUnlock()\n\n    if b.closed {\n        return omnistorage.ErrBackendClosed\n    }\n    return nil\n}\n</code></pre>"},{"location":"guides/custom-backend/#step-6-register-the-backend","title":"Step 6: Register the Backend","text":"<pre><code>func init() {\n    omnistorage.Register(\"mycloud\", NewFromConfig)\n}\n\nfunc NewFromConfig(config map[string]string) (omnistorage.Backend, error) {\n    return New(Config{\n        Bucket:   config[\"bucket\"],\n        APIKey:   config[\"api_key\"],\n        Endpoint: config[\"endpoint\"],\n    })\n}\n</code></pre>"},{"location":"guides/custom-backend/#extended-backend","title":"Extended Backend","text":"<p>For advanced features, implement <code>ExtendedBackend</code>:</p> <pre><code>type ExtendedBackend interface {\n    Backend\n    Stat(ctx context.Context, path string) (ObjectInfo, error)\n    Mkdir(ctx context.Context, path string) error\n    Rmdir(ctx context.Context, path string) error\n    Copy(ctx context.Context, src, dst string) error\n    Move(ctx context.Context, src, dst string) error\n    Features() Features\n}\n</code></pre>"},{"location":"guides/custom-backend/#implementing-stat","title":"Implementing Stat","text":"<pre><code>func (b *Backend) Stat(ctx context.Context, path string) (omnistorage.ObjectInfo, error) {\n    if err := b.checkClosed(); err != nil {\n        return nil, err\n    }\n\n    meta, err := b.client.GetMetadata(path)\n    if err != nil {\n        if isNotFoundError(err) {\n            return nil, omnistorage.ErrNotFound\n        }\n        return nil, err\n    }\n\n    return &amp;objectInfo{\n        name:    path,\n        size:    meta.Size,\n        modTime: meta.ModTime,\n    }, nil\n}\n\ntype objectInfo struct {\n    name    string\n    size    int64\n    modTime time.Time\n}\n\nfunc (o *objectInfo) Name() string              { return o.name }\nfunc (o *objectInfo) Size() int64               { return o.size }\nfunc (o *objectInfo) ModTime() time.Time        { return o.modTime }\nfunc (o *objectInfo) IsDir() bool               { return false }\nfunc (o *objectInfo) Hash(t omnistorage.HashType) string { return \"\" }\nfunc (o *objectInfo) MimeType() string          { return \"\" }\nfunc (o *objectInfo) Metadata() map[string]string { return nil }\n</code></pre>"},{"location":"guides/custom-backend/#implementing-features","title":"Implementing Features","text":"<pre><code>func (b *Backend) Features() omnistorage.Features {\n    return omnistorage.Features{\n        Copy:           true,  // Server-side copy supported\n        Move:           true,  // Server-side move supported\n        Purge:          false, // Recursive delete not supported\n        SetModTime:     false,\n        CustomMetadata: true,\n    }\n}\n</code></pre>"},{"location":"guides/custom-backend/#testing","title":"Testing","text":"<p>Create conformance tests:</p> <pre><code>func TestBackendConformance(t *testing.T) {\n    backend, _ := mycloud.New(testConfig)\n    defer backend.Close()\n\n    ctx := context.Background()\n\n    t.Run(\"WriteRead\", func(t *testing.T) {\n        data := []byte(\"test data\")\n\n        w, _ := backend.NewWriter(ctx, \"test.txt\")\n        w.Write(data)\n        w.Close()\n\n        r, _ := backend.NewReader(ctx, \"test.txt\")\n        result, _ := io.ReadAll(r)\n        r.Close()\n\n        if !bytes.Equal(result, data) {\n            t.Errorf(\"got %q, want %q\", result, data)\n        }\n    })\n\n    // More tests...\n}\n</code></pre>"},{"location":"guides/custom-backend/#best-practices","title":"Best Practices","text":"<ol> <li>Handle context cancellation - Check <code>ctx.Err()</code> in long operations</li> <li>Use standard errors - Return <code>omnistorage.ErrNotFound</code>, etc.</li> <li>Make delete idempotent - Return nil for non-existent paths</li> <li>Implement proper closing - Release resources in <code>Close()</code></li> <li>Thread safety - Use mutexes for shared state</li> <li>Register in init() - For automatic registration</li> </ol>"},{"location":"guides/multi-writer/","title":"Multi-Writer Guide","text":"<p>The multi package provides fan-out writing to multiple backends simultaneously.</p>"},{"location":"guides/multi-writer/#overview","title":"Overview","text":"<p>Write the same data to multiple storage backends at once:</p> <ul> <li>Replication across storage providers</li> <li>Writing to both local and remote storage</li> <li>Backup during write operations</li> <li>Testing with multiple backends</li> </ul>"},{"location":"guides/multi-writer/#basic-usage","title":"Basic Usage","text":"<pre><code>import \"github.com/grokify/omnistorage/multi\"\n\n// Create backends\nlocal := file.New(file.Config{Root: \"/data\"})\ns3Backend, _ := s3.New(s3.Config{Bucket: \"my-bucket\"})\ngcsBackend, _ := gcs.New(gcs.Config{Bucket: \"my-bucket\"})\n\n// Create multi-writer\nmw, err := multi.NewWriter(local, s3Backend, gcsBackend)\nif err != nil {\n    log.Fatal(err)\n}\n\n// Write to all backends simultaneously\nw, _ := mw.NewWriter(ctx, \"data/file.json\")\nw.Write([]byte(`{\"key\": \"value\"}`))\nw.Close()\n</code></pre>"},{"location":"guides/multi-writer/#write-modes","title":"Write Modes","text":""},{"location":"guides/multi-writer/#writeall-default","title":"WriteAll (Default)","text":"<p>All backends must succeed. If any backend fails, the entire write fails.</p> <pre><code>mw, _ := multi.NewWriterWithOptions(\n    []omnistorage.Backend{b1, b2, b3},\n    multi.WithMode(multi.WriteAll),\n)\n</code></pre> <p>Use when: Data must be in all backends or none (strong consistency).</p>"},{"location":"guides/multi-writer/#writebesteffort","title":"WriteBestEffort","text":"<p>Write to all backends but continue on failure. Errors are collected and returned.</p> <pre><code>mw, _ := multi.NewWriterWithOptions(\n    []omnistorage.Backend{b1, b2, b3},\n    multi.WithMode(multi.WriteBestEffort),\n)\n\nw, _ := mw.NewWriter(ctx, \"file.txt\")\n_, _ = w.Write(data)\nerr := w.Close()\n\n// Check for partial failures\nif err != nil {\n    if me, ok := err.(*multi.MultiError); ok {\n        for _, e := range me.All() {\n            log.Printf(\"Backend error: %v\", e)\n        }\n    }\n}\n</code></pre> <p>Use when: Some backends can fail without blocking the operation.</p>"},{"location":"guides/multi-writer/#writequorum","title":"WriteQuorum","text":"<p>Requires a majority of backends to succeed.</p> <pre><code>mw, _ := multi.NewWriterWithOptions(\n    []omnistorage.Backend{b1, b2, b3}, // 3 backends\n    multi.WithMode(multi.WriteQuorum),\n)\n\n// Write succeeds if 2+ backends succeed\nw, _ := mw.NewWriter(ctx, \"file.txt\")\n</code></pre> <p>Use when: Fault tolerance with majority agreement (similar to distributed systems).</p>"},{"location":"guides/multi-writer/#error-handling","title":"Error Handling","text":"<p>The multi-writer returns <code>*MultiError</code> when multiple errors occur:</p> <pre><code>w, err := mw.NewWriter(ctx, \"file.txt\")\nif err != nil {\n    if me, ok := err.(*multi.MultiError); ok {\n        // Multiple errors\n        fmt.Printf(\"First error: %v\\n\", me.Error())\n        for _, e := range me.All() {\n            fmt.Printf(\"- %v\\n\", e)\n        }\n    }\n    return err\n}\n</code></pre>"},{"location":"guides/multi-writer/#multierror-methods","title":"MultiError Methods","text":"<pre><code>type MultiError struct {\n    Errors []error\n}\n\nfunc (e *MultiError) Error() string     // First error + \"(and more errors)\"\nfunc (e *MultiError) Unwrap() error     // First error (for errors.Is/As)\nfunc (e *MultiError) All() []error      // All errors\n</code></pre>"},{"location":"guides/multi-writer/#backend-count","title":"Backend Count","text":"<p>Check the number of active backends:</p> <pre><code>count := mw.Backends()\nfmt.Printf(\"Writing to %d backends\\n\", count)\n</code></pre>"},{"location":"guides/multi-writer/#use-cases","title":"Use Cases","text":""},{"location":"guides/multi-writer/#local-cloud-backup","title":"Local + Cloud Backup","text":"<p>Write to local storage and backup to cloud simultaneously:</p> <pre><code>local := file.New(file.Config{Root: \"/data\"})\ncloud, _ := s3.New(s3.Config{Bucket: \"backups\"})\n\nmw, _ := multi.NewWriterWithOptions(\n    []omnistorage.Backend{local, cloud},\n    multi.WithMode(multi.WriteBestEffort), // Continue if cloud fails\n)\n\n// Data is written locally and backed up to cloud\nw, _ := mw.NewWriter(ctx, \"important.dat\")\n</code></pre>"},{"location":"guides/multi-writer/#multi-region-replication","title":"Multi-Region Replication","text":"<p>Write to multiple regions for availability:</p> <pre><code>usEast, _ := s3.New(s3.Config{Bucket: \"data\", Region: \"us-east-1\"})\nusWest, _ := s3.New(s3.Config{Bucket: \"data\", Region: \"us-west-2\"})\neuWest, _ := s3.New(s3.Config{Bucket: \"data\", Region: \"eu-west-1\"})\n\nmw, _ := multi.NewWriterWithOptions(\n    []omnistorage.Backend{usEast, usWest, euWest},\n    multi.WithMode(multi.WriteQuorum), // 2 of 3 must succeed\n)\n</code></pre>"},{"location":"guides/multi-writer/#test-production","title":"Test + Production","text":"<p>Write to both test and production backends:</p> <pre><code>prod, _ := s3.New(prodConfig)\ntest := memory.New() // In-memory for inspection\n\nmw, _ := multi.NewWriter(prod, test)\n\n// After writing, can inspect test backend\n</code></pre>"},{"location":"guides/multi-writer/#nil-backend-handling","title":"Nil Backend Handling","text":"<p>Nil backends are automatically filtered:</p> <pre><code>var optionalBackend omnistorage.Backend // may be nil\n\nmw, err := multi.NewWriter(\n    requiredBackend,\n    optionalBackend, // Ignored if nil\n)\n// mw has 1 backend if optionalBackend is nil\n</code></pre>"},{"location":"guides/multi-writer/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the right mode - WriteAll for consistency, WriteBestEffort for availability</li> <li>Handle MultiError - Check for partial failures in best-effort mode</li> <li>Close writers - Ensures all backends complete their writes</li> <li>Consider latency - Writes complete when the slowest backend finishes</li> </ol>"},{"location":"reference/errors/","title":"Errors Reference","text":"<p>This page documents all error types in omnistorage.</p>"},{"location":"reference/errors/#standard-errors","title":"Standard Errors","text":"<p>Omnistorage defines standard errors for common cases:</p> <pre><code>var (\n    // ErrNotFound is returned when a path does not exist.\n    ErrNotFound = errors.New(\"omnistorage: not found\")\n\n    // ErrAlreadyExists is returned when a path already exists (if applicable).\n    ErrAlreadyExists = errors.New(\"omnistorage: already exists\")\n\n    // ErrPermissionDenied is returned when access is denied.\n    ErrPermissionDenied = errors.New(\"omnistorage: permission denied\")\n\n    // ErrBackendClosed is returned when operating on a closed backend.\n    ErrBackendClosed = errors.New(\"omnistorage: backend closed\")\n\n    // ErrInvalidPath is returned when a path is invalid or empty.\n    ErrInvalidPath = errors.New(\"omnistorage: invalid path\")\n\n    // ErrWriterClosed is returned when writing to a closed writer.\n    ErrWriterClosed = errors.New(\"omnistorage: writer closed\")\n\n    // ErrReaderClosed is returned when reading from a closed reader.\n    ErrReaderClosed = errors.New(\"omnistorage: reader closed\")\n)\n</code></pre>"},{"location":"reference/errors/#checking-errors","title":"Checking Errors","text":"<p>Use <code>errors.Is()</code> to check for specific errors:</p> <pre><code>r, err := backend.NewReader(ctx, \"file.txt\")\nif err != nil {\n    if errors.Is(err, omnistorage.ErrNotFound) {\n        log.Println(\"File not found\")\n        return nil\n    }\n    if errors.Is(err, omnistorage.ErrPermissionDenied) {\n        log.Println(\"Access denied\")\n        return err\n    }\n    return err\n}\n</code></pre>"},{"location":"reference/errors/#error-scenarios","title":"Error Scenarios","text":""},{"location":"reference/errors/#errnotfound","title":"ErrNotFound","text":"<p>Returned when a file or path doesn't exist:</p> <pre><code>// Reading a non-existent file\nr, err := backend.NewReader(ctx, \"missing.txt\")\n// err == omnistorage.ErrNotFound\n\n// Stat on non-existent path\ninfo, err := ext.Stat(ctx, \"missing.txt\")\n// err == omnistorage.ErrNotFound\n</code></pre>"},{"location":"reference/errors/#errbackendclosed","title":"ErrBackendClosed","text":"<p>Returned when operating on a closed backend:</p> <pre><code>backend.Close()\n\n// Any operation after close\n_, err := backend.NewWriter(ctx, \"file.txt\")\n// err == omnistorage.ErrBackendClosed\n</code></pre>"},{"location":"reference/errors/#errinvalidpath","title":"ErrInvalidPath","text":"<p>Returned when a path is empty or invalid:</p> <pre><code>// Empty path\n_, err := backend.NewWriter(ctx, \"\")\n// err == omnistorage.ErrInvalidPath\n</code></pre>"},{"location":"reference/errors/#errwriterclosed","title":"ErrWriterClosed","text":"<p>Returned when writing to a closed writer:</p> <pre><code>w, _ := backend.NewWriter(ctx, \"file.txt\")\nw.Close()\n\n_, err := w.Write([]byte(\"data\"))\n// err == omnistorage.ErrWriterClosed\n</code></pre>"},{"location":"reference/errors/#errreaderclosed","title":"ErrReaderClosed","text":"<p>Returned when reading from a closed reader:</p> <pre><code>r, _ := backend.NewReader(ctx, \"file.txt\")\nr.Close()\n\n_, err := r.Read(buf)\n// err == omnistorage.ErrReaderClosed\n</code></pre>"},{"location":"reference/errors/#multi-writer-errors","title":"Multi-Writer Errors","text":"<p>The multi-writer returns <code>*MultiError</code> when multiple errors occur:</p> <pre><code>type MultiError struct {\n    Errors []error\n}\n\nfunc (e *MultiError) Error() string   // First error message\nfunc (e *MultiError) Unwrap() error   // First error (for errors.Is)\nfunc (e *MultiError) All() []error    // All errors\n</code></pre>"},{"location":"reference/errors/#handling-multierror","title":"Handling MultiError","text":"<pre><code>w, err := mw.NewWriter(ctx, \"file.txt\")\nif err != nil {\n    if me, ok := err.(*multi.MultiError); ok {\n        fmt.Println(\"Multiple errors occurred:\")\n        for _, e := range me.All() {\n            fmt.Printf(\"  - %v\\n\", e)\n        }\n    }\n    return err\n}\n</code></pre>"},{"location":"reference/errors/#checking-wrapped-errors","title":"Checking Wrapped Errors","text":"<pre><code>// errors.Is works with the first wrapped error\nif errors.Is(err, omnistorage.ErrPermissionDenied) {\n    // First error was permission denied\n}\n</code></pre>"},{"location":"reference/errors/#sync-errors","title":"Sync Errors","text":""},{"location":"reference/errors/#retryerror","title":"RetryError","text":"<p>Returned when all retries are exhausted:</p> <pre><code>type RetryError struct {\n    Attempts int\n    LastErr  error\n}\n\nfunc (e *RetryError) Error() string\nfunc (e *RetryError) Unwrap() error\n</code></pre>"},{"location":"reference/errors/#handling-retry-errors","title":"Handling Retry Errors","text":"<pre><code>result, err := sync.Sync(ctx, src, dst, \"\", \"\", opts)\nif err != nil {\n    var retryErr *sync.RetryError\n    if errors.As(err, &amp;retryErr) {\n        fmt.Printf(\"Failed after %d attempts: %v\\n\",\n            retryErr.Attempts, retryErr.LastErr)\n    }\n}\n</code></pre>"},{"location":"reference/errors/#context-errors","title":"Context Errors","text":"<p>Backend operations respect context cancellation:</p> <pre><code>ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\ndefer cancel()\n\n// Operation may return context.DeadlineExceeded\nr, err := backend.NewReader(ctx, \"large-file.dat\")\nif errors.Is(err, context.DeadlineExceeded) {\n    log.Println(\"Operation timed out\")\n}\n\n// Or context.Canceled if cancelled\nif errors.Is(err, context.Canceled) {\n    log.Println(\"Operation was cancelled\")\n}\n</code></pre>"},{"location":"reference/errors/#best-practices","title":"Best Practices","text":"<ol> <li>Use errors.Is() - Don't compare errors directly with <code>==</code></li> <li>Check specific errors first - Handle ErrNotFound before generic errors</li> <li>Wrap errors with context - Use <code>fmt.Errorf(\"failed to read: %w\", err)</code></li> <li>Handle MultiError - Check <code>All()</code> for complete error list</li> <li>Respect context - Pass context to all operations</li> </ol>"},{"location":"reference/interfaces/","title":"Interfaces Reference","text":"<p>This page documents all public interfaces in omnistorage.</p>"},{"location":"reference/interfaces/#backend","title":"Backend","text":"<p>The core interface for all storage backends.</p> <pre><code>type Backend interface {\n    // NewWriter creates a writer for the given path/key.\n    // The returned writer must be closed after use.\n    NewWriter(ctx context.Context, path string, opts ...WriterOption) (io.WriteCloser, error)\n\n    // NewReader creates a reader for the given path/key.\n    // Returns ErrNotFound if the path does not exist.\n    NewReader(ctx context.Context, path string, opts ...ReaderOption) (io.ReadCloser, error)\n\n    // Exists checks if a path exists.\n    Exists(ctx context.Context, path string) (bool, error)\n\n    // Delete removes a path.\n    // Returns nil if the path does not exist (idempotent).\n    Delete(ctx context.Context, path string) error\n\n    // List lists paths with the given prefix.\n    // Returns an empty slice if no paths match.\n    List(ctx context.Context, prefix string) ([]string, error)\n\n    // Close releases any resources held by the backend.\n    Close() error\n}\n</code></pre>"},{"location":"reference/interfaces/#usage","title":"Usage","text":"<pre><code>backend := file.New(file.Config{Root: \"/data\"})\ndefer backend.Close()\n\n// Write\nw, _ := backend.NewWriter(ctx, \"file.txt\")\nw.Write([]byte(\"data\"))\nw.Close()\n\n// Read\nr, _ := backend.NewReader(ctx, \"file.txt\")\ndata, _ := io.ReadAll(r)\nr.Close()\n\n// Check existence\nexists, _ := backend.Exists(ctx, \"file.txt\")\n\n// List\nfiles, _ := backend.List(ctx, \"prefix/\")\n\n// Delete\nbackend.Delete(ctx, \"file.txt\")\n</code></pre>"},{"location":"reference/interfaces/#extendedbackend","title":"ExtendedBackend","text":"<p>Extended interface for metadata and server-side operations.</p> <pre><code>type ExtendedBackend interface {\n    Backend\n\n    // Stat returns metadata for the path.\n    Stat(ctx context.Context, path string) (ObjectInfo, error)\n\n    // Mkdir creates a directory.\n    Mkdir(ctx context.Context, path string) error\n\n    // Rmdir removes an empty directory.\n    Rmdir(ctx context.Context, path string) error\n\n    // Copy copies a file from src to dst (server-side when possible).\n    Copy(ctx context.Context, src, dst string) error\n\n    // Move moves a file from src to dst (server-side when possible).\n    Move(ctx context.Context, src, dst string) error\n\n    // Features returns the backend's capabilities.\n    Features() Features\n}\n</code></pre>"},{"location":"reference/interfaces/#usage_1","title":"Usage","text":"<pre><code>// Check if backend supports extended operations\nif ext, ok := omnistorage.AsExtended(backend); ok {\n    // Get metadata\n    info, _ := ext.Stat(ctx, \"file.txt\")\n    fmt.Printf(\"Size: %d\\n\", info.Size())\n\n    // Server-side copy\n    if ext.Features().Copy {\n        ext.Copy(ctx, \"src.txt\", \"dst.txt\")\n    }\n\n    // Directory operations\n    ext.Mkdir(ctx, \"new-folder\")\n    ext.Rmdir(ctx, \"empty-folder\")\n}\n</code></pre>"},{"location":"reference/interfaces/#objectinfo","title":"ObjectInfo","text":"<p>Metadata for a file or object.</p> <pre><code>type ObjectInfo interface {\n    // Name returns the base name of the file.\n    Name() string\n\n    // Size returns the file size in bytes.\n    Size() int64\n\n    // ModTime returns the modification time.\n    ModTime() time.Time\n\n    // IsDir returns true if this is a directory.\n    IsDir() bool\n\n    // Hash returns the hash of the specified type, or empty string if unavailable.\n    Hash(HashType) string\n\n    // MimeType returns the MIME type, or empty string if unknown.\n    MimeType() string\n\n    // Metadata returns custom metadata key-value pairs.\n    Metadata() map[string]string\n}\n</code></pre>"},{"location":"reference/interfaces/#usage_2","title":"Usage","text":"<pre><code>info, _ := ext.Stat(ctx, \"file.txt\")\n\nfmt.Printf(\"Name: %s\\n\", info.Name())\nfmt.Printf(\"Size: %d bytes\\n\", info.Size())\nfmt.Printf(\"Modified: %s\\n\", info.ModTime())\nfmt.Printf(\"Is Directory: %v\\n\", info.IsDir())\nfmt.Printf(\"MD5: %s\\n\", info.Hash(omnistorage.HashMD5))\nfmt.Printf(\"Content-Type: %s\\n\", info.MimeType())\n</code></pre>"},{"location":"reference/interfaces/#features","title":"Features","text":"<p>Backend capability flags.</p> <pre><code>type Features struct {\n    Copy           bool // Server-side copy\n    Move           bool // Server-side move\n    Purge          bool // Recursive delete\n    SetModTime     bool // Set modification time\n    CustomMetadata bool // Custom metadata support\n}\n</code></pre>"},{"location":"reference/interfaces/#usage_3","title":"Usage","text":"<pre><code>features := ext.Features()\n\nif features.Copy {\n    // Use efficient server-side copy\n    ext.Copy(ctx, src, dst)\n} else {\n    // Fall back to read + write\n    omnistorage.CopyPath(ctx, backend, src, backend, dst)\n}\n</code></pre>"},{"location":"reference/interfaces/#recordwriter","title":"RecordWriter","text":"<p>For streaming record-oriented data.</p> <pre><code>type RecordWriter interface {\n    // Write writes a single record.\n    Write(data []byte) error\n\n    // Flush flushes buffered data.\n    Flush() error\n\n    // Close flushes and closes the writer.\n    Close() error\n}\n</code></pre>"},{"location":"reference/interfaces/#usage_4","title":"Usage","text":"<pre><code>import \"github.com/grokify/omnistorage/format/ndjson\"\n\nw, _ := backend.NewWriter(ctx, \"records.ndjson\")\nwriter := ndjson.NewWriter(w)\n\nwriter.Write([]byte(`{\"id\":1}`))\nwriter.Write([]byte(`{\"id\":2}`))\nwriter.Flush() // Flush buffered data\nwriter.Close()\n</code></pre>"},{"location":"reference/interfaces/#recordreader","title":"RecordReader","text":"<p>For reading record-oriented data.</p> <pre><code>type RecordReader interface {\n    // Read reads the next record.\n    // Returns io.EOF when no more records are available.\n    Read() ([]byte, error)\n\n    // Close closes the reader.\n    Close() error\n}\n</code></pre>"},{"location":"reference/interfaces/#usage_5","title":"Usage","text":"<pre><code>r, _ := backend.NewReader(ctx, \"records.ndjson\")\nreader := ndjson.NewReader(r)\ndefer reader.Close()\n\nfor {\n    record, err := reader.Read()\n    if err == io.EOF {\n        break\n    }\n    if err != nil {\n        return err\n    }\n    process(record)\n}\n</code></pre>"},{"location":"reference/interfaces/#hashtype","title":"HashType","text":"<p>Supported hash types for checksums.</p> <pre><code>type HashType int\n\nconst (\n    HashNone HashType = iota\n    HashMD5\n    HashSHA1\n    HashSHA256\n    HashCRC32\n)\n</code></pre>"},{"location":"reference/interfaces/#usage_6","title":"Usage","text":"<pre><code>info, _ := ext.Stat(ctx, \"file.txt\")\n\nmd5 := info.Hash(omnistorage.HashMD5)\nsha256 := info.Hash(omnistorage.HashSHA256)\n</code></pre>"},{"location":"reference/interfaces/#backendfactory","title":"BackendFactory","text":"<p>Factory function for creating backends from configuration.</p> <pre><code>type BackendFactory func(config map[string]string) (Backend, error)\n</code></pre>"},{"location":"reference/interfaces/#usage_7","title":"Usage","text":"<pre><code>// Register a factory\nomnistorage.Register(\"mybackend\", func(config map[string]string) (omnistorage.Backend, error) {\n    return mybackend.New(mybackend.Config{\n        Setting: config[\"setting\"],\n    })\n})\n\n// Open using the factory\nbackend, _ := omnistorage.Open(\"mybackend\", map[string]string{\n    \"setting\": \"value\",\n})\n</code></pre>"},{"location":"reference/options/","title":"Options Reference","text":"<p>This page documents all option types in omnistorage.</p>"},{"location":"reference/options/#writeroption","title":"WriterOption","text":"<p>Options for creating writers.</p> <pre><code>type WriterOption func(*WriterConfig)\n\ntype WriterConfig struct {\n    BufferSize  int               // Buffer size in bytes (0 = default)\n    ContentType string            // MIME type hint\n    Metadata    map[string]string // Backend-specific metadata\n}\n</code></pre>"},{"location":"reference/options/#available-options","title":"Available Options","text":"<pre><code>// Set content type\nomnistorage.WithContentType(\"application/json\")\n\n// Set custom metadata\nomnistorage.WithMetadata(map[string]string{\n    \"author\": \"john\",\n    \"version\": \"1.0\",\n})\n\n// Set buffer size\nomnistorage.WithBufferSize(64 * 1024) // 64 KB\n</code></pre>"},{"location":"reference/options/#usage","title":"Usage","text":"<pre><code>w, _ := backend.NewWriter(ctx, \"data.json\",\n    omnistorage.WithContentType(\"application/json\"),\n    omnistorage.WithMetadata(map[string]string{\n        \"source\": \"api\",\n    }),\n)\n</code></pre>"},{"location":"reference/options/#readeroption","title":"ReaderOption","text":"<p>Options for creating readers.</p> <pre><code>type ReaderOption func(*ReaderConfig)\n\ntype ReaderConfig struct {\n    BufferSize int   // Buffer size in bytes (0 = default)\n    Offset     int64 // Start reading from offset (if supported)\n    Limit      int64 // Maximum bytes to read (0 = no limit)\n}\n</code></pre>"},{"location":"reference/options/#available-options_1","title":"Available Options","text":"<pre><code>// Set buffer size\nomnistorage.WithReaderBufferSize(64 * 1024)\n\n// Set offset (range read)\nomnistorage.WithOffset(1024)\n\n// Set limit\nomnistorage.WithLimit(4096)\n</code></pre>"},{"location":"reference/options/#usage_1","title":"Usage","text":"<pre><code>// Read bytes 1024-5120\nr, _ := backend.NewReader(ctx, \"large-file.dat\",\n    omnistorage.WithOffset(1024),\n    omnistorage.WithLimit(4096),\n)\n</code></pre>"},{"location":"reference/options/#sync-options","title":"Sync Options","text":"<p>Options for sync operations.</p> <pre><code>type Options struct {\n    // Comparison\n    DeleteExtra   bool // Delete files in dst not in src\n    Checksum      bool // Compare by checksum vs modtime/size\n    SizeOnly      bool // Compare by size only\n    IgnoreTime    bool // Ignore modification time\n    IgnoreSize    bool // Ignore size differences\n\n    // Behavior\n    DryRun         bool // Report changes without making them\n    IgnoreExisting bool // Skip files that exist in destination\n    MaxErrors      int  // Stop after N errors (0 = first error)\n\n    // Transfer controls\n    Concurrency    int              // Parallel transfers (default: 4)\n    BandwidthLimit int64            // Rate limit in bytes/second\n    Retry          *RetryConfig     // Retry configuration\n    Progress       func(Progress)   // Progress callback\n\n    // Filtering\n    Filter         *filter.Filter   // Include/exclude filter\n    DeleteExcluded bool             // Delete excluded files from dst\n\n    // Metadata\n    PreserveMetadata *MetadataOptions // Metadata preservation\n}\n</code></pre>"},{"location":"reference/options/#common-configurations","title":"Common Configurations","text":"<pre><code>// Mirror sync (make dst match src exactly)\nsync.Options{\n    DeleteExtra: true,\n}\n\n// Safe copy (don't delete, skip existing)\nsync.Options{\n    IgnoreExisting: true,\n}\n\n// Checksum verification\nsync.Options{\n    Checksum: true,\n}\n\n// Dry run preview\nsync.Options{\n    DryRun: true,\n}\n\n// Full featured\nsync.Options{\n    DeleteExtra:    true,\n    Checksum:       true,\n    Concurrency:    8,\n    BandwidthLimit: 10 * 1024 * 1024, // 10 MB/s\n    Progress: func(p sync.Progress) {\n        fmt.Printf(\"%s: %d/%d\\n\", p.Phase, p.FilesTransferred, p.TotalFiles)\n    },\n}\n</code></pre>"},{"location":"reference/options/#retryconfig","title":"RetryConfig","text":"<p>Configuration for automatic retries.</p> <pre><code>type RetryConfig struct {\n    MaxRetries   int           // Maximum retry attempts\n    InitialDelay time.Duration // Initial delay between retries\n    MaxDelay     time.Duration // Maximum delay\n    Multiplier   float64       // Delay multiplier for exponential backoff\n    Jitter       float64       // Random jitter factor (0-1)\n}\n</code></pre>"},{"location":"reference/options/#default-configuration","title":"Default Configuration","text":"<pre><code>func DefaultRetryConfig() RetryConfig {\n    return RetryConfig{\n        MaxRetries:   3,\n        InitialDelay: time.Second,\n        MaxDelay:     30 * time.Second,\n        Multiplier:   2.0,\n        Jitter:       0.1,\n    }\n}\n</code></pre>"},{"location":"reference/options/#usage_2","title":"Usage","text":"<pre><code>retryConfig := sync.DefaultRetryConfig()\nretryConfig.MaxRetries = 5\nretryConfig.MaxDelay = time.Minute\n\nresult, _ := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    Retry: &amp;retryConfig,\n})\n</code></pre>"},{"location":"reference/options/#metadataoptions","title":"MetadataOptions","text":"<p>Options for metadata preservation.</p> <pre><code>type MetadataOptions struct {\n    ContentType    bool // Preserve MIME type\n    CustomMetadata bool // Preserve custom metadata\n    ModTime        bool // Preserve modification time\n}\n</code></pre>"},{"location":"reference/options/#default-configuration_1","title":"Default Configuration","text":"<pre><code>func DefaultMetadataOptions() MetadataOptions {\n    return MetadataOptions{\n        ContentType:    true,\n        CustomMetadata: true,\n        ModTime:        false,\n    }\n}\n</code></pre>"},{"location":"reference/options/#usage_3","title":"Usage","text":"<pre><code>result, _ := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    PreserveMetadata: &amp;sync.MetadataOptions{\n        ContentType:    true,\n        CustomMetadata: true,\n        ModTime:        true, // Requires SetModTime support\n    },\n})\n</code></pre>"},{"location":"reference/options/#filter-options","title":"Filter Options","text":"<p>Options for creating filters.</p> <pre><code>// Pattern matching\nfilter.Include(\"*.json\")\nfilter.Exclude(\"*.tmp\")\n\n// Size filters\nfilter.MinSize(1024)       // Minimum 1 KB\nfilter.MaxSize(100 * MB)   // Maximum 100 MB\n\n// Age filters\nfilter.MinAge(24 * time.Hour)  // Older than 1 day\nfilter.MaxAge(7 * 24 * time.Hour) // Newer than 7 days\n</code></pre>"},{"location":"reference/options/#usage_4","title":"Usage","text":"<pre><code>f := filter.New(\n    filter.Include(\"*.json\"),\n    filter.Exclude(\"test_*.json\"),\n    filter.MinSize(100),\n    filter.MaxAge(30 * 24 * time.Hour),\n)\n\nresult, _ := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    Filter: f,\n})\n</code></pre>"},{"location":"reference/options/#channel-backend-options","title":"Channel Backend Options","text":"<p>Options for the channel backend.</p> <pre><code>// Set channel buffer size\nchannel.WithBufferSize(100) // Default: 100\n\n// Enable persistence (buffer for late readers)\nchannel.WithPersistence(true) // Default: false\n</code></pre>"},{"location":"reference/options/#usage_5","title":"Usage","text":"<pre><code>backend := channel.New(\n    channel.WithBufferSize(50),\n    channel.WithPersistence(true),\n)\n</code></pre>"},{"location":"reference/options/#multi-writer-options","title":"Multi-Writer Options","text":"<p>Options for multi-writer.</p> <pre><code>type WriteMode int\n\nconst (\n    WriteAll        WriteMode = iota // All backends must succeed\n    WriteBestEffort                   // Continue on failure\n    WriteQuorum                       // Majority must succeed\n)\n\n// Set write mode\nmulti.WithMode(multi.WriteQuorum)\n</code></pre>"},{"location":"reference/options/#usage_6","title":"Usage","text":"<pre><code>mw, _ := multi.NewWriterWithOptions(\n    []omnistorage.Backend{b1, b2, b3},\n    multi.WithMode(multi.WriteBestEffort),\n)\n</code></pre>"},{"location":"releases/v0.1.0/","title":"v0.1.0 Release Notes","text":"<p>Release Date: 2026-01-10</p> <p>Omnistorage v0.1.0 is the initial release of a unified storage abstraction layer for Go, inspired by rclone. It provides a single interface for reading and writing to various storage backends with composable layers for compression and record framing.</p>"},{"location":"releases/v0.1.0/#highlights","title":"Highlights","text":"<ul> <li>Unified Interface - Single API for multiple storage backends</li> <li>5 Backends - File, Memory, S3, SFTP, and Channel</li> <li>Sync Engine - rclone-inspired file synchronization with ~95% feature parity</li> <li>Composable Layers - Compression (gzip, zstd) and format (NDJSON) wrappers</li> <li>Extended Interface - Optional metadata, server-side copy/move, and capability discovery</li> </ul>"},{"location":"releases/v0.1.0/#installation","title":"Installation","text":"<pre><code>go get github.com/grokify/omnistorage@v0.1.0\n</code></pre>"},{"location":"releases/v0.1.0/#whats-included","title":"What's Included","text":""},{"location":"releases/v0.1.0/#core-interfaces","title":"Core Interfaces","text":"Interface Description <code>Backend</code> Core read/write interface with NewWriter, NewReader, Exists, Delete, List, Close <code>ExtendedBackend</code> Adds Stat, Mkdir, Rmdir, Copy, Move, and Features methods <code>RecordWriter</code> Line/record-oriented writing for streaming data <code>RecordReader</code> Line/record-oriented reading for streaming data <code>ObjectInfo</code> File metadata (Size, ModTime, Hash, ContentType) <code>Features</code> Backend capability discovery"},{"location":"releases/v0.1.0/#backends","title":"Backends","text":"Backend Package Extended Description File <code>backend/file</code> Yes Local filesystem storage Memory <code>backend/memory</code> Yes In-memory storage for testing S3 <code>backend/s3</code> Yes AWS S3, Cloudflare R2, MinIO, Wasabi SFTP <code>backend/sftp</code> Yes SSH file transfer with password/key auth Channel <code>backend/channel</code> No Go channel for inter-goroutine streaming"},{"location":"releases/v0.1.0/#compression","title":"Compression","text":"Format Package Description Gzip <code>compress/gzip</code> Standard gzip compression Zstandard <code>compress/zstd</code> High-performance Zstd compression"},{"location":"releases/v0.1.0/#format-layers","title":"Format Layers","text":"Format Package Description NDJSON <code>format/ndjson</code> Newline-delimited JSON record framing"},{"location":"releases/v0.1.0/#sync-engine","title":"Sync Engine","text":"<p>The sync package provides rclone-like file synchronization:</p> Function Description <code>Sync()</code> One-way sync (mirror destination to source) <code>Copy()</code> Copy files without deleting extras <code>Bisync()</code> Bidirectional sync with conflict resolution <code>Check()</code> Compare files between backends <code>Verify()</code> Verify file integrity <p>Sync Features:</p> <ul> <li>Parallel transfers with configurable concurrency</li> <li>Bandwidth limiting (token bucket algorithm)</li> <li>Retry with exponential backoff and jitter</li> <li>Progress callbacks for real-time status</li> <li>Dry-run mode for testing</li> <li>Structured logging via slog</li> </ul> <p>Filtering:</p> <ul> <li>Include/exclude patterns (glob syntax)</li> <li>Size filters (MinSize, MaxSize)</li> <li>Age filters (MinAge, MaxAge)</li> <li>Filter from file</li> </ul> <p>Conflict Resolution (Bisync):</p> <ul> <li><code>NewerWins</code> - Newer file overwrites older</li> <li><code>LargerWins</code> - Larger file overwrites smaller</li> <li><code>SourceWins</code> - First backend always wins</li> <li><code>DestWins</code> - Second backend always wins</li> <li><code>KeepBoth</code> - Keep both with conflict suffix</li> <li><code>Skip</code> - Skip conflicting files</li> <li><code>Error</code> - Report as error</li> </ul>"},{"location":"releases/v0.1.0/#multi-writer","title":"Multi-Writer","text":"<p>Fan-out writing to multiple backends with three modes:</p> <ul> <li><code>WriteAll</code> - All backends must succeed</li> <li><code>WriteBestEffort</code> - Continue on failures</li> <li><code>WriteQuorum</code> - Majority must succeed</li> </ul>"},{"location":"releases/v0.1.0/#utilities","title":"Utilities","text":"Function Description <code>CopyPath()</code> Copy between any two backends <code>MovePath()</code> Move by copy-then-delete <code>SmartMove()</code> Server-side move with fallback <code>AsExtended()</code> Safe type assertion to ExtendedBackend"},{"location":"releases/v0.1.0/#quick-start","title":"Quick Start","text":""},{"location":"releases/v0.1.0/#basic-readwrite","title":"Basic Read/Write","text":"<pre><code>import (\n    \"context\"\n    \"github.com/grokify/omnistorage/backend/file\"\n)\n\nfunc main() {\n    ctx := context.Background()\n    backend := file.New(file.Config{Root: \"/data\"})\n    defer backend.Close()\n\n    // Write\n    w, _ := backend.NewWriter(ctx, \"hello.txt\")\n    w.Write([]byte(\"Hello, World!\"))\n    w.Close()\n\n    // Read\n    r, _ := backend.NewReader(ctx, \"hello.txt\")\n    data, _ := io.ReadAll(r)\n    r.Close()\n}\n</code></pre>"},{"location":"releases/v0.1.0/#with-compression","title":"With Compression","text":"<pre><code>import (\n    \"github.com/grokify/omnistorage/backend/file\"\n    \"github.com/grokify/omnistorage/compress/gzip\"\n)\n\n// Write compressed\nw, _ := backend.NewWriter(ctx, \"data.gz\")\ngz, _ := gzip.NewWriter(w)\ngz.Write([]byte(\"compressed content\"))\ngz.Close()\n</code></pre>"},{"location":"releases/v0.1.0/#sync-between-backends","title":"Sync Between Backends","text":"<pre><code>import \"github.com/grokify/omnistorage/sync\"\n\nresult, err := sync.Sync(ctx, srcBackend, dstBackend, \"data/\", \"backup/\", sync.Options{\n    DeleteExtra: true,\n    Concurrency: 4,\n    Progress: func(p sync.Progress) {\n        fmt.Printf(\"%s: %d/%d\\n\", p.Phase, p.FilesTransferred, p.TotalFiles)\n    },\n})\n</code></pre>"},{"location":"releases/v0.1.0/#using-the-registry","title":"Using the Registry","text":"<pre><code>import (\n    \"github.com/grokify/omnistorage\"\n    _ \"github.com/grokify/omnistorage/backend/file\"\n    _ \"github.com/grokify/omnistorage/backend/s3\"\n)\n\n// Open by name from configuration\nbackend, _ := omnistorage.Open(\"s3\", map[string]string{\n    \"bucket\": \"my-bucket\",\n    \"region\": \"us-east-1\",\n})\n</code></pre>"},{"location":"releases/v0.1.0/#external-backends","title":"External Backends","text":"<p>Some backends are in separate repositories to minimize dependencies:</p> Backend Repository Google Drive omnistorage-google Google Cloud Storage omnistorage-google (planned)"},{"location":"releases/v0.1.0/#breaking-changes","title":"Breaking Changes","text":"<p>None - this is the initial release.</p>"},{"location":"releases/v0.1.0/#known-limitations","title":"Known Limitations","text":"<ul> <li>SFTP host key verification is disabled by default (configure <code>KnownHostsFile</code> for production)</li> <li>Dropbox backend is not yet implemented (config only)</li> <li>No CLI tool yet (planned for v1.1.0)</li> </ul>"},{"location":"releases/v0.1.0/#documentation","title":"Documentation","text":"<ul> <li>README</li> <li>API Reference</li> <li>Full Documentation</li> </ul>"},{"location":"releases/v0.1.0/#whats-next","title":"What's Next","text":"<p>See the Roadmap for planned features:</p> <ul> <li>v0.2.0 - Consumer cloud backends (Dropbox, OneDrive)</li> <li>v0.3.0 - Security &amp; authentication</li> <li>v0.4.0 - Observability (metrics, tracing)</li> <li>v1.0.0 - Stable API</li> <li>v1.1.0 - CLI tool</li> </ul>"},{"location":"releases/v0.1.0/#contributing","title":"Contributing","text":"<p>Contributions welcome! Priority areas:</p> <ol> <li>New backends (follow <code>backend/file</code> as template)</li> <li>Tests (especially integration tests)</li> <li>Documentation improvements</li> <li>Bug fixes</li> </ol>"},{"location":"releases/v0.1.0/#license","title":"License","text":"<p>MIT License - see LICENSE</p>"},{"location":"sync/","title":"Sync Engine Overview","text":"<p>The sync package provides rclone-like file synchronization between storage backends.</p>"},{"location":"sync/#features","title":"Features","text":"<ul> <li>Sync - Make destination match source (with optional deletes)</li> <li>Copy - Copy files without deleting extras</li> <li>Move - Move files from source to destination</li> <li>Check - Verify files match between backends</li> <li>Filtering - Include/exclude patterns, size/age filters</li> <li>Transfer Controls - Bandwidth limiting, parallel transfers, retry</li> </ul>"},{"location":"sync/#quick-example","title":"Quick Example","text":"<pre><code>import \"github.com/grokify/omnistorage/sync\"\n\nsrcBackend := file.New(file.Config{Root: \"/local\"})\ndstBackend, _ := s3.New(s3.Config{Bucket: \"my-bucket\"})\n\n// Sync local to S3\nresult, err := sync.Sync(ctx, srcBackend, dstBackend, \"data/\", \"backup/\", sync.Options{\n    DeleteExtra: true,  // Delete files in dst not in src\n    Progress: func(p sync.Progress) {\n        fmt.Printf(\"%s: %d/%d files\\n\", p.Phase, p.FilesTransferred, p.TotalFiles)\n    },\n})\n\nfmt.Printf(\"Copied: %d, Updated: %d, Deleted: %d\\n\",\n    result.Copied, result.Updated, result.Deleted)\n</code></pre>"},{"location":"sync/#core-operations","title":"Core Operations","text":"Operation Function Description Sync <code>sync.Sync()</code> Make dst match src (like <code>rclone sync</code>) Copy <code>sync.Copy()</code> Copy without deleting extras Move <code>sync.Move()</code> Move files (copy + delete source) Check <code>sync.Check()</code> Compare and report differences Verify <code>sync.Verify()</code> Verify files match"},{"location":"sync/#options","title":"Options","text":"<pre><code>sync.Options{\n    DeleteExtra:      true,   // Delete extra files in destination\n    DryRun:           true,   // Report without making changes\n    Checksum:         true,   // Compare by checksum\n    SizeOnly:         true,   // Compare by size only\n    IgnoreExisting:   true,   // Skip existing files\n    Concurrency:      4,      // Parallel transfers\n    BandwidthLimit:   1&lt;&lt;20,  // 1 MB/s rate limit\n    MaxErrors:        10,     // Stop after N errors\n    Progress:         func(Progress){}, // Progress callback\n    Filter:           filter, // Include/exclude filter\n    Retry:            &amp;RetryConfig{},   // Retry configuration\n    PreserveMetadata: &amp;MetadataOptions{}, // Metadata preservation\n}\n</code></pre>"},{"location":"sync/#rclone-parity","title":"rclone Parity","text":"<p>The sync package implements ~95% of rclone's core sync features:</p> <ul> <li>All core operations (sync, copy, move, check)</li> <li>All comparison methods (size, modtime, checksum)</li> <li>Filtering (include, exclude, size, age)</li> <li>Transfer controls (parallel, bandwidth, retry)</li> </ul> <p>See rclone Parity for detailed comparison.</p>"},{"location":"sync/#documentation","title":"Documentation","text":"<ul> <li>Operations - Detailed operation reference</li> <li>Filtering - Include/exclude patterns</li> <li>Transfer Controls - Bandwidth, concurrency, retry</li> <li>rclone Parity - Feature comparison with rclone</li> </ul>"},{"location":"sync/filtering/","title":"Filtering","text":"<p>The filter package provides include/exclude patterns, size filters, and age filters for sync operations.</p>"},{"location":"sync/filtering/#basic-usage","title":"Basic Usage","text":"<pre><code>import \"github.com/grokify/omnistorage/sync/filter\"\n\nf := filter.New(\n    filter.Include(\"*.json\"),\n    filter.Exclude(\"*.tmp\"),\n)\n\nresult, err := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    Filter: f,\n})\n</code></pre>"},{"location":"sync/filtering/#pattern-matching","title":"Pattern Matching","text":""},{"location":"sync/filtering/#include-patterns","title":"Include Patterns","text":"<p>Include only files matching patterns:</p> <pre><code>f := filter.New(\n    filter.Include(\"*.json\"),\n    filter.Include(\"*.yaml\"),\n    filter.Include(\"data/**\"),\n)\n</code></pre>"},{"location":"sync/filtering/#exclude-patterns","title":"Exclude Patterns","text":"<p>Exclude files matching patterns:</p> <pre><code>f := filter.New(\n    filter.Exclude(\"*.tmp\"),\n    filter.Exclude(\"*.bak\"),\n    filter.Exclude(\".git/**\"),\n    filter.Exclude(\"node_modules/**\"),\n)\n</code></pre>"},{"location":"sync/filtering/#pattern-syntax","title":"Pattern Syntax","text":"Pattern Matches <code>*.json</code> Any file ending in <code>.json</code> <code>*.{json,yaml}</code> Files ending in <code>.json</code> or <code>.yaml</code> <code>data/*</code> Files directly in <code>data/</code> directory <code>data/**</code> All files under <code>data/</code> recursively <code>??.txt</code> Two-character name + <code>.txt</code> <code>[abc].txt</code> <code>a.txt</code>, <code>b.txt</code>, or <code>c.txt</code>"},{"location":"sync/filtering/#combining-includeexclude","title":"Combining Include/Exclude","text":"<p>When both include and exclude patterns are specified:</p> <ol> <li>Include patterns are checked first</li> <li>Then exclude patterns filter the results</li> </ol> <pre><code>f := filter.New(\n    filter.Include(\"*.json\"),      // Include JSON files\n    filter.Exclude(\"test_*.json\"), // But exclude test files\n)\n</code></pre>"},{"location":"sync/filtering/#size-filters","title":"Size Filters","text":"<p>Filter by file size:</p> <pre><code>f := filter.New(\n    filter.MinSize(1024),           // At least 1 KB\n    filter.MaxSize(100 * 1024 * 1024), // At most 100 MB\n)\n</code></pre>"},{"location":"sync/filtering/#size-constants","title":"Size Constants","text":"<pre><code>const (\n    KB = 1024\n    MB = 1024 * KB\n    GB = 1024 * MB\n)\n\nf := filter.New(\n    filter.MinSize(10 * KB),\n    filter.MaxSize(1 * GB),\n)\n</code></pre>"},{"location":"sync/filtering/#age-filters","title":"Age Filters","text":"<p>Filter by modification time:</p> <pre><code>f := filter.New(\n    filter.MinAge(24 * time.Hour),    // Older than 1 day\n    filter.MaxAge(7 * 24 * time.Hour), // Newer than 7 days\n)\n</code></pre>"},{"location":"sync/filtering/#common-age-filters","title":"Common Age Filters","text":"<pre><code>// Files modified in the last hour\nfilter.MaxAge(time.Hour)\n\n// Files older than 30 days\nfilter.MinAge(30 * 24 * time.Hour)\n\n// Files between 1 and 7 days old\nfilter.New(\n    filter.MinAge(24 * time.Hour),\n    filter.MaxAge(7 * 24 * time.Hour),\n)\n</code></pre>"},{"location":"sync/filtering/#filter-from-file","title":"Filter From File","text":"<p>Load filters from a file:</p> <pre><code>f, err := filter.FromFile(\"filters.txt\")\nif err != nil {\n    log.Fatal(err)\n}\n</code></pre>"},{"location":"sync/filtering/#filter-file-format","title":"Filter File Format","text":"<pre><code># Comments start with #\n# Include patterns (prefix with +)\n+ *.json\n+ *.yaml\n+ data/**\n\n# Exclude patterns (prefix with -)\n- *.tmp\n- *.bak\n- .git/**\n- node_modules/**\n\n# Size filters\n--min-size 1K\n--max-size 100M\n\n# Age filters\n--min-age 1d\n--max-age 7d\n</code></pre>"},{"location":"sync/filtering/#size-units-in-files","title":"Size Units in Files","text":"Unit Value <code>B</code> Bytes <code>K</code> Kilobytes <code>M</code> Megabytes <code>G</code> Gigabytes"},{"location":"sync/filtering/#age-units-in-files","title":"Age Units in Files","text":"Unit Value <code>s</code> Seconds <code>m</code> Minutes <code>h</code> Hours <code>d</code> Days <code>w</code> Weeks <code>M</code> Months (30 days) <code>y</code> Years (365 days)"},{"location":"sync/filtering/#delete-excluded","title":"Delete Excluded","text":"<p>Delete files in destination that match exclude patterns:</p> <pre><code>result, err := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    Filter: f,\n    DeleteExcluded: true, // Delete excluded files from destination\n})\n</code></pre>"},{"location":"sync/filtering/#combined-example","title":"Combined Example","text":"<pre><code>f := filter.New(\n    // Include patterns\n    filter.Include(\"*.json\"),\n    filter.Include(\"*.yaml\"),\n    filter.Include(\"config/**\"),\n\n    // Exclude patterns\n    filter.Exclude(\"*.tmp\"),\n    filter.Exclude(\"*.bak\"),\n    filter.Exclude(\".git/**\"),\n    filter.Exclude(\"test_*.json\"),\n\n    // Size limits\n    filter.MinSize(100),        // At least 100 bytes\n    filter.MaxSize(10 * MB),    // At most 10 MB\n\n    // Age limits\n    filter.MaxAge(30 * 24 * time.Hour), // Modified in last 30 days\n)\n\nresult, err := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    Filter: f,\n    DeleteExtra: true,\n})\n</code></pre>"},{"location":"sync/filtering/#checking-filter-matches","title":"Checking Filter Matches","text":"<pre><code>f := filter.New(filter.Include(\"*.json\"))\n\n// Check if a file passes the filter\nif f.Match(\"data.json\", info) {\n    fmt.Println(\"File passes filter\")\n}\n</code></pre>"},{"location":"sync/operations/","title":"Sync Operations","text":"<p>This page covers all sync operations in detail.</p>"},{"location":"sync/operations/#sync","title":"Sync","text":"<p>Make destination match source, including deletes.</p> <pre><code>result, err := sync.Sync(ctx, srcBackend, dstBackend, \"src/\", \"dst/\", sync.Options{\n    DeleteExtra: true,  // Delete files in dst not in src\n})\n</code></pre>"},{"location":"sync/operations/#behavior","title":"Behavior","text":"<ol> <li>Lists all files in source and destination</li> <li>Copies new/modified files from source to destination</li> <li>Optionally deletes files in destination not in source</li> <li>Returns detailed results</li> </ol>"},{"location":"sync/operations/#result","title":"Result","text":"<pre><code>type Result struct {\n    Copied    int   // Files copied\n    Updated   int   // Files updated\n    Deleted   int   // Files deleted\n    Skipped   int   // Files skipped\n    Errors    int   // Error count\n    BytesCopied int64 // Total bytes transferred\n}\n</code></pre>"},{"location":"sync/operations/#copy","title":"Copy","text":"<p>Copy files without deleting extras.</p> <pre><code>// Copy a directory\nresult, err := sync.Copy(ctx, src, dst, \"data/\", \"backup/\", sync.Options{})\n\n// Copy a single file\nerr := sync.CopyFile(ctx, src, dst, \"file.txt\", \"file_copy.txt\")\n</code></pre>"},{"location":"sync/operations/#convenience-functions","title":"Convenience Functions","text":"<pre><code>// Copy between paths (same or different backends)\nsync.CopyBetweenPaths(ctx, srcBackend, \"src/path\", dstBackend, \"dst/path\", opts)\n\n// Copy to a path on same backend\nsync.CopyToPath(ctx, backend, \"src/file.txt\", \"dst/file.txt\")\n\n// Copy from path with path transformation\nsync.CopyFromPath(ctx, src, dst, \"source/\", \"dest/\", opts)\n\n// Copy preserving full tree structure\nsync.TreeCopy(ctx, src, dst, \"source/\", \"dest/\", opts)\n</code></pre>"},{"location":"sync/operations/#copy-with-progress","title":"Copy with Progress","text":"<pre><code>result, err := sync.CopyWithProgress(ctx, src, dst, \"data/\", \"backup/\",\n    func(file string, bytes int64) {\n        fmt.Printf(\"Copying %s: %d bytes\\n\", file, bytes)\n    })\n</code></pre>"},{"location":"sync/operations/#move","title":"Move","text":"<p>Move files from source to destination (copy + delete source).</p> <pre><code>result, err := sync.Move(ctx, src, dst, \"old/\", \"new/\", sync.Options{})\n</code></pre>"},{"location":"sync/operations/#behavior_1","title":"Behavior","text":"<ol> <li>Copies files to destination</li> <li>Deletes source files after successful copy</li> <li>Uses server-side move when available</li> </ol>"},{"location":"sync/operations/#check","title":"Check","text":"<p>Compare files between backends and report differences.</p> <pre><code>result, err := sync.Check(ctx, src, dst, \"data/\", \"backup/\", sync.Options{\n    Checksum: true,  // Compare by checksum\n})\n\nfmt.Printf(\"Match: %d\\n\", len(result.Match))\nfmt.Printf(\"Differ: %d\\n\", len(result.Differ))\nfmt.Printf(\"SrcOnly: %d\\n\", len(result.SrcOnly))\nfmt.Printf(\"DstOnly: %d\\n\", len(result.DstOnly))\n</code></pre>"},{"location":"sync/operations/#checkresult","title":"CheckResult","text":"<pre><code>type CheckResult struct {\n    Match   []string // Files that match\n    Differ  []string // Files that differ\n    SrcOnly []string // Files only in source\n    DstOnly []string // Files only in destination\n}\n</code></pre>"},{"location":"sync/operations/#diff","title":"Diff","text":"<p>Get human-readable differences:</p> <pre><code>diff := sync.Diff(ctx, src, dst, \"data/\", \"backup/\", sync.Options{})\nfor _, d := range diff {\n    fmt.Println(d)\n}\n</code></pre>"},{"location":"sync/operations/#verify","title":"Verify","text":"<p>Verify files match between backends.</p> <pre><code>// Simple verify (returns bool)\ninSync, err := sync.Verify(ctx, src, dst, \"data/\", \"backup/\", sync.Options{})\nif inSync {\n    fmt.Println(\"All files match\")\n}\n\n// Verify single file\nmatch, err := sync.VerifyFile(ctx, src, dst, \"file.txt\", sync.Options{})\n\n// Verify with checksum\nmatch, err := sync.VerifyChecksum(ctx, src, dst, \"file.txt\")\n</code></pre>"},{"location":"sync/operations/#detailed-verification","title":"Detailed Verification","text":"<pre><code>// Get detailed results\ndetails, err := sync.VerifyWithDetails(ctx, src, dst, \"data/\", \"backup/\", sync.Options{})\nfmt.Printf(\"Total: %d, Match: %d, Differ: %d\\n\",\n    details.Total, details.Matched, details.Different)\n\n// Human-readable report\nreport, err := sync.VerifyAndReport(ctx, src, dst, \"data/\", \"backup/\", sync.Options{})\nfmt.Println(report)\n</code></pre>"},{"location":"sync/operations/#integrity-verification","title":"Integrity Verification","text":"<p>Verify file integrity (checksum validation):</p> <pre><code>// Verify single file integrity\nvalid, err := sync.VerifyIntegrity(ctx, backend, \"file.txt\", expectedHash)\n\n// Verify all files\nresults, err := sync.VerifyAllIntegrity(ctx, backend, \"data/\", hashMap)\n</code></pre>"},{"location":"sync/operations/#comparison-methods","title":"Comparison Methods","text":"<p>Control how files are compared:</p> <pre><code>sync.Options{\n    // Default: Size + ModTime\n    // Files match if size and modification time are equal\n\n    Checksum: true,\n    // Compare by checksum (MD5/SHA256)\n    // Slower but more accurate\n\n    SizeOnly: true,\n    // Compare by size only\n    // Fast but less accurate\n\n    IgnoreTime: true,\n    // Ignore modification time differences\n\n    IgnoreSize: true,\n    // Ignore size differences (use with Checksum)\n}\n</code></pre>"},{"location":"sync/operations/#dry-run","title":"Dry Run","text":"<p>Preview changes without making them:</p> <pre><code>result, err := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    DryRun: true,\n})\n// result shows what WOULD happen, but no changes are made\n</code></pre>"},{"location":"sync/operations/#progress-tracking","title":"Progress Tracking","text":"<pre><code>result, err := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    Progress: func(p sync.Progress) {\n        fmt.Printf(\"[%s] %d/%d files, %d/%d bytes\\n\",\n            p.Phase,\n            p.FilesTransferred, p.TotalFiles,\n            p.BytesTransferred, p.TotalBytes)\n    },\n})\n</code></pre>"},{"location":"sync/operations/#progress-fields","title":"Progress Fields","text":"<pre><code>type Progress struct {\n    Phase            string // \"scanning\", \"copying\", \"deleting\"\n    CurrentFile      string // Current file being processed\n    TotalFiles       int    // Total files to process\n    FilesTransferred int    // Files completed\n    TotalBytes       int64  // Total bytes to transfer\n    BytesTransferred int64  // Bytes transferred\n}\n</code></pre>"},{"location":"sync/rclone-parity/","title":"Rclone Feature Parity","text":"<p>This document tracks omnistorage's feature parity with rclone, the popular cloud storage sync tool that inspired omnistorage's sync package.</p>"},{"location":"sync/rclone-parity/#overview","title":"Overview","text":"<p>Omnistorage aims to provide rclone-like functionality as a Go library. While rclone is a CLI tool with 70+ backends, omnistorage focuses on providing a clean programmatic API for the most common sync operations.</p> <p>Current Parity: ~95% of core features</p>"},{"location":"sync/rclone-parity/#feature-comparison","title":"Feature Comparison","text":""},{"location":"sync/rclone-parity/#core-operations","title":"Core Operations","text":"Feature rclone omnistorage Status Sync (mirror) <code>rclone sync</code> <code>sync.Sync()</code> \u2705 Complete Copy (no delete) <code>rclone copy</code> <code>sync.Copy()</code> \u2705 Complete Move <code>rclone move</code> <code>sync.Move()</code> \u2705 Complete Check/Verify <code>rclone check</code> <code>sync.Check()</code>, <code>sync.Verify()</code> \u2705 Complete List files <code>rclone ls</code> <code>backend.List()</code> \u2705 Complete Delete <code>rclone delete</code> <code>backend.Delete()</code> \u2705 Complete Mkdir <code>rclone mkdir</code> <code>ext.Mkdir()</code> \u2705 Complete Rmdir <code>rclone rmdir</code> <code>ext.Rmdir()</code> \u2705 Complete"},{"location":"sync/rclone-parity/#comparison-methods","title":"Comparison Methods","text":"Feature rclone omnistorage Status Size + ModTime Default Default \u2705 Complete Checksum <code>--checksum</code> <code>Options{Checksum: true}</code> \u2705 Complete Size only <code>--size-only</code> <code>Options{SizeOnly: true}</code> \u2705 Complete Ignore time <code>--ignore-times</code> <code>Options{IgnoreTime: true}</code> \u2705 Complete Ignore size <code>--ignore-size</code> <code>Options{IgnoreSize: true}</code> \u2705 Complete"},{"location":"sync/rclone-parity/#safety-control","title":"Safety &amp; Control","text":"Feature rclone omnistorage Status Dry run <code>--dry-run</code> <code>Options{DryRun: true}</code> \u2705 Complete Progress callbacks <code>-P/--progress</code> <code>Options{Progress: func()}</code> \u2705 Complete Context cancellation Ctrl+C <code>context.Context</code> \u2705 Complete Max errors <code>--max-errors</code> <code>Options{MaxErrors: N}</code> \u2705 Complete Skip existing <code>--ignore-existing</code> <code>Options{IgnoreExisting: true}</code> \u2705 Complete"},{"location":"sync/rclone-parity/#server-side-operations","title":"Server-Side Operations","text":"Feature rclone omnistorage Status Server-side copy Auto-detected Auto via <code>Features().Copy</code> \u2705 Complete Server-side move Auto-detected Auto via <code>Features().Move</code> \u2705 Complete"},{"location":"sync/rclone-parity/#transfer-controls","title":"Transfer Controls","text":"Feature rclone omnistorage Status Parallel transfers <code>--transfers N</code> <code>Options{Concurrency: N}</code> \u2705 Complete Bandwidth limiting <code>--bwlimit</code> <code>Options{BandwidthLimit: N}</code> \u2705 Complete Retry on error <code>--retries</code> <code>Options{Retry: &amp;RetryConfig{}}</code> \u2705 Complete Check-first mode <code>--check-first</code> Default behavior \u2705 Complete"},{"location":"sync/rclone-parity/#filtering","title":"Filtering","text":"Feature rclone omnistorage Status Include patterns <code>--include</code> <code>Options{Filter: ...}</code> \u2705 Complete Exclude patterns <code>--exclude</code> <code>Options{Filter: ...}</code> \u2705 Complete Min/max size <code>--min-size/--max-size</code> <code>filter.MinSize/MaxSize</code> \u2705 Complete Min/max age <code>--min-age/--max-age</code> <code>filter.MinAge/MaxAge</code> \u2705 Complete Filter from file <code>--filter-from</code> <code>filter.FromFile()</code> \u2705 Complete Delete excluded <code>--delete-excluded</code> <code>Options{DeleteExcluded: true}</code> \u2705 Complete"},{"location":"sync/rclone-parity/#advanced-features","title":"Advanced Features","text":"Feature rclone omnistorage Status Bidirectional sync <code>rclone bisync</code> - \u274c Not planned for v1.0 Interactive mode <code>-i</code> - \u274c Not applicable (library) Metadata preservation <code>--metadata</code> <code>Options{PreserveMetadata: ...}</code> \u2705 Complete Deduplication <code>rclone dedupe</code> - \u274c Not implemented"},{"location":"sync/rclone-parity/#usage-examples","title":"Usage Examples","text":""},{"location":"sync/rclone-parity/#rclone-vs-omnistorage","title":"rclone vs omnistorage","text":"<p>Sync with delete: <pre><code># rclone\nrclone sync /local/path s3:bucket/path --delete-during\n\n# omnistorage\nresult, _ := sync.Sync(ctx, localBackend, s3Backend, \"path\", \"path\", sync.Options{\n    DeleteExtra: true,\n})\n</code></pre></p> <p>Copy with progress: <pre><code># rclone\nrclone copy /local/path s3:bucket/path -P\n\n# omnistorage\nresult, _ := sync.Copy(ctx, src, dst, \"path\", \"path\", sync.Options{\n    Progress: func(p sync.Progress) {\n        fmt.Printf(\"%s: %d/%d files\\n\", p.Phase, p.FilesTransferred, p.TotalFiles)\n    },\n})\n</code></pre></p> <p>Dry run with filters: <pre><code># rclone\nrclone sync /local s3:bucket --dry-run --include \"*.json\" --exclude \"*.tmp\"\n\n# omnistorage\nresult, _ := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    DryRun: true,\n    Filter: filter.New(\n        filter.Include(\"*.json\"),\n        filter.Exclude(\"*.tmp\"),\n    ),\n})\n</code></pre></p> <p>Checksum verification: <pre><code># rclone\nrclone check /local s3:bucket --checksum\n\n# omnistorage\nresult, _ := sync.Check(ctx, src, dst, \"\", \"\", sync.Options{\n    Checksum: true,\n})\n</code></pre></p> <p>Bandwidth limiting: <pre><code># rclone\nrclone sync /local s3:bucket --bwlimit 1M\n\n# omnistorage\nresult, _ := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    BandwidthLimit: 1024 * 1024, // 1MB/s\n})\n</code></pre></p> <p>Retry on failure: <pre><code># rclone\nrclone sync /local s3:bucket --retries 3\n\n# omnistorage\nretryConfig := sync.DefaultRetryConfig()\nretryConfig.MaxRetries = 3\nresult, _ := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    Retry: &amp;retryConfig,\n})\n</code></pre></p> <p>Preserve metadata: <pre><code># rclone\nrclone sync /local s3:bucket --metadata\n\n# omnistorage\nresult, _ := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    PreserveMetadata: &amp;sync.MetadataOptions{\n        ContentType:    true,\n        CustomMetadata: true,\n    },\n})\n</code></pre></p>"},{"location":"sync/rclone-parity/#implementation-priority","title":"Implementation Priority","text":""},{"location":"sync/rclone-parity/#high-priority-v020","title":"High Priority (v0.2.0)","text":"<ol> <li>~~Parallel transfers~~ \u2705</li> <li>~~Filtering system~~ \u2705</li> <li>~~Move in sync package~~ \u2705</li> </ol>"},{"location":"sync/rclone-parity/#medium-priority-v030","title":"Medium Priority (v0.3.0)","text":"<ol> <li>~~Bandwidth limiting~~ \u2705</li> <li>~~Retry/resume support~~ \u2705</li> <li>~~Extended metadata preservation~~ \u2705</li> </ol>"},{"location":"sync/rclone-parity/#low-priority-v10","title":"Low Priority (v1.0+)","text":"<ol> <li>Deduplication</li> <li>Bidirectional sync</li> </ol>"},{"location":"sync/rclone-parity/#references","title":"References","text":"<ul> <li>rclone sync documentation</li> <li>rclone copy documentation</li> <li>rclone filtering documentation</li> <li>rclone global flags</li> </ul>"},{"location":"sync/transfer-controls/","title":"Transfer Controls","text":"<p>Control transfer behavior with bandwidth limiting, parallel transfers, and retry configuration.</p>"},{"location":"sync/transfer-controls/#parallel-transfers","title":"Parallel Transfers","text":"<p>Control the number of concurrent file transfers:</p> <pre><code>result, err := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    Concurrency: 8, // 8 parallel transfers (default: 4)\n})\n</code></pre>"},{"location":"sync/transfer-controls/#guidelines","title":"Guidelines","text":"Network Recommended Concurrency Local/LAN 8-16 High-speed Internet 4-8 Slow Internet 2-4 Rate-limited APIs 1-2"},{"location":"sync/transfer-controls/#bandwidth-limiting","title":"Bandwidth Limiting","text":"<p>Limit transfer speed with a token bucket rate limiter:</p> <pre><code>result, err := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    BandwidthLimit: 1024 * 1024, // 1 MB/s\n})\n</code></pre>"},{"location":"sync/transfer-controls/#common-limits","title":"Common Limits","text":"<pre><code>// 1 MB/s\nBandwidthLimit: 1 * 1024 * 1024\n\n// 10 MB/s\nBandwidthLimit: 10 * 1024 * 1024\n\n// 100 KB/s\nBandwidthLimit: 100 * 1024\n</code></pre>"},{"location":"sync/transfer-controls/#how-it-works","title":"How It Works","text":"<p>The bandwidth limiter uses a token bucket algorithm:</p> <ul> <li>Tokens represent bytes that can be transferred</li> <li>Tokens are added at the specified rate</li> <li>Transfers wait for tokens when the bucket is empty</li> <li>The bucket can burst up to the limit</li> </ul>"},{"location":"sync/transfer-controls/#retry-configuration","title":"Retry Configuration","text":"<p>Configure automatic retries for failed operations:</p> <pre><code>retryConfig := sync.DefaultRetryConfig()\nretryConfig.MaxRetries = 5\nretryConfig.InitialDelay = time.Second\nretryConfig.MaxDelay = 30 * time.Second\n\nresult, err := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    Retry: &amp;retryConfig,\n})\n</code></pre>"},{"location":"sync/transfer-controls/#retryconfig","title":"RetryConfig","text":"<pre><code>type RetryConfig struct {\n    MaxRetries   int           // Maximum retry attempts (default: 3)\n    InitialDelay time.Duration // Initial delay (default: 1s)\n    MaxDelay     time.Duration // Maximum delay (default: 30s)\n    Multiplier   float64       // Delay multiplier (default: 2.0)\n    Jitter       float64       // Random jitter (default: 0.1)\n}\n</code></pre>"},{"location":"sync/transfer-controls/#default-configuration","title":"Default Configuration","text":"<pre><code>func DefaultRetryConfig() RetryConfig {\n    return RetryConfig{\n        MaxRetries:   3,\n        InitialDelay: time.Second,\n        MaxDelay:     30 * time.Second,\n        Multiplier:   2.0,\n        Jitter:       0.1,\n    }\n}\n</code></pre>"},{"location":"sync/transfer-controls/#exponential-backoff","title":"Exponential Backoff","text":"<p>Delays increase exponentially with jitter:</p> Attempt Base Delay With Jitter (\u00b110%) 1 1s 0.9s - 1.1s 2 2s 1.8s - 2.2s 3 4s 3.6s - 4.4s 4 8s 7.2s - 8.8s 5 16s 14.4s - 17.6s"},{"location":"sync/transfer-controls/#retryable-errors","title":"Retryable Errors","text":"<p>By default, these errors are retried:</p> <ul> <li>Network timeouts</li> <li>Connection resets</li> <li>HTTP 429 (Too Many Requests)</li> <li>HTTP 500, 502, 503, 504 (Server errors)</li> </ul>"},{"location":"sync/transfer-controls/#max-errors","title":"Max Errors","text":"<p>Stop sync after a number of errors:</p> <pre><code>result, err := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    MaxErrors: 10, // Stop after 10 errors (0 = stop on first)\n})\n</code></pre>"},{"location":"sync/transfer-controls/#progress-tracking","title":"Progress Tracking","text":"<p>Monitor transfer progress:</p> <pre><code>result, err := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    Progress: func(p sync.Progress) {\n        percent := float64(p.BytesTransferred) / float64(p.TotalBytes) * 100\n        fmt.Printf(\"\\r[%s] %.1f%% (%d/%d files)\",\n            p.Phase, percent, p.FilesTransferred, p.TotalFiles)\n    },\n})\n</code></pre>"},{"location":"sync/transfer-controls/#progress-fields","title":"Progress Fields","text":"<pre><code>type Progress struct {\n    Phase            string // \"scanning\", \"copying\", \"deleting\"\n    CurrentFile      string // Current file being processed\n    TotalFiles       int    // Total files to process\n    FilesTransferred int    // Files completed\n    TotalBytes       int64  // Total bytes to transfer\n    BytesTransferred int64  // Bytes transferred\n}\n</code></pre>"},{"location":"sync/transfer-controls/#metadata-preservation","title":"Metadata Preservation","text":"<p>Preserve file metadata during transfers:</p> <pre><code>result, err := sync.Sync(ctx, src, dst, \"\", \"\", sync.Options{\n    PreserveMetadata: &amp;sync.MetadataOptions{\n        ContentType:    true, // Preserve MIME type\n        CustomMetadata: true, // Preserve custom metadata\n        ModTime:        true, // Preserve modification time\n    },\n})\n</code></pre>"},{"location":"sync/transfer-controls/#default-metadata-options","title":"Default Metadata Options","text":"<pre><code>func DefaultMetadataOptions() MetadataOptions {\n    return MetadataOptions{\n        ContentType:    true,\n        CustomMetadata: true,\n        ModTime:        false, // Requires ExtendedBackend with SetModTime\n    }\n}\n</code></pre>"},{"location":"sync/transfer-controls/#combined-example","title":"Combined Example","text":"<pre><code>retryConfig := sync.DefaultRetryConfig()\nretryConfig.MaxRetries = 5\n\nresult, err := sync.Sync(ctx, src, dst, \"data/\", \"backup/\", sync.Options{\n    // Transfer controls\n    Concurrency:    8,              // 8 parallel transfers\n    BandwidthLimit: 10 * 1024 * 1024, // 10 MB/s limit\n    Retry:          &amp;retryConfig,   // Retry configuration\n    MaxErrors:      100,            // Continue despite errors\n\n    // Progress\n    Progress: func(p sync.Progress) {\n        fmt.Printf(\"[%s] %d/%d files\\n\", p.Phase, p.FilesTransferred, p.TotalFiles)\n    },\n\n    // Metadata\n    PreserveMetadata: sync.DefaultMetadataOptions(),\n})\n</code></pre>"}]}